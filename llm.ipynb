{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.0.8)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.1.29)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.0.25)\n",
      "Requirement already satisfied: langchain in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.1.10)\n",
      "Requirement already satisfied: pypdf in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: chromadb in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.4.24)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (0.2.2)\n",
      "Collecting cohere\n",
      "  Downloading cohere-4.52-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-openai) (1.13.3)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (6.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (4.3.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (0.1.17)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-core) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-community) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.1.1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.7.3)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.110.0)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.27.1)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (4.10.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (3.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.17.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.23.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (4.66.2)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (6.1.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (1.62.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (4.1.2)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (29.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (4.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from chromadb) (3.9.15)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from cohere) (2.2.1)\n",
      "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
      "  Using cached fastavro-1.9.4-cp310-cp310-win_amd64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from cohere) (6.11.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from anyio<5,>=3->langchain-core) (1.2.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.37.0,>=0.36.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.36.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
      "Requirement already satisfied: certifi>=14.05.14 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.28.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (4.25.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.23.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.23.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.44b0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.44b0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.44b0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.44b0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.44b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (58.1.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.7.2)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pydantic<3,>=1->langchain-core) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from requests<3,>=2->langchain-core) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.21.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.4.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\lmore\\onedrive\\documents\\github\\csc-4351-llm\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\n",
      "Downloading cohere-4.52-py3-none-any.whl (52 kB)\n",
      "   ---------------------------------------- 0.0/52.0 kB ? eta -:--:--\n",
      "   ---------------------------------------  51.2/52.0 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 52.0/52.0 kB 889.0 kB/s eta 0:00:00\n",
      "Using cached fastavro-1.9.4-cp310-cp310-win_amd64.whl (497 kB)\n",
      "Installing collected packages: fastavro, cohere\n",
      "Successfully installed cohere-4.52 fastavro-1.9.4\n"
     ]
    }
   ],
   "source": [
    "#must have python 3.10 installed\n",
    "!pip install pandas langchain-openai langchain-core langchain-community langchain pypdf chromadb rank-bm25 cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_openai_api_key = \"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\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"fgNPrLh4cGje0EBy7GL6P4pHR6NbNkjobUgsFcIF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Loading and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"Compilers Overview.txt\"\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "file_paths = [\n",
    "    \"./CSC 4351 Midterm Notes.pdf\",\n",
    "    \"./overview.pdf\",\n",
    "    \"./lexical.pdf\",\n",
    "    \"./parsing.pdf\"\n",
    "]\n",
    "page_contents = []\n",
    "for file_path in file_paths:\n",
    "    pdf_loader = PyPDFLoader(file_path)\n",
    "    docs = pdf_loader.load()\n",
    "    page_contents += [doc.page_content for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"as an introduction to compilers let's have a look at the compiler architecture in general so if you look at a tool like say gcc the new c compiler it is not just a compiler it's it cons it drives multiple different tools so first of all the source code is not actual c source code instead it is c source code together with preprocessor macros like pound include pound define etc which could be called maybe skeletal source code and this is fed to the c preprocessor which then produces the actual source code now comes uh comes the compiler which translates the source code not to an object file not to an executable but to assembly text why because we already have an assembler in our system we already have uh other tools that can uh take it from the assembly text file all the way to an executable it is much easier to to generate assembly text than to generate an executable the assembler then assembles the code and produces a relocatable machine code relocatable means that the assembler produces object an object file for each individual source code but the object file is generated in the way that that it does not depend on the final memory location where in the executable the object uh that object file is going to reside so for example any branch instructions uh are going to be relative branch instructions relative to the current uh location uh or to the current program counter a jump plus a minus a certain number of bytes in um one direction or another instead of jumping to an addre to an actual uh fixed address uh in memory the reason for this is that then later in in the linker these different object files can be combined into a single executable and then eventually the loader loads that object file loads that executable into an operating systems process now if we zoom in a little bit what does the compiler look like the compiler reads read source code generates assembly and in between there's an intermediate representation and the compiler is written such that they can be multiple different frontends the front end is language specific the for the job of the front end is to analyze the source code and figure out what does it mean and the back end is machine specific that is it generates assembly code for the target architecture a compiler like gcc has a variety of different frontends and back ends gcc has on the order of 10 or 12 front ends c c plus plus objective c also known as objectionable c d yet another successor to see pascal fortran 77 fortran 90 um java and ada and probably another couple languages and the gcc has a back end for just about every processor that ever existed or is currently out there so in the order of uh 50 or so back-ends optimized for different processors and to make this work the intermediate representation needs to be language and architecture independent next if we look at this in a little bit more detail the compiler consists of multiple different phases uh if the analysis part is the first three or four boxes here in this vertical column of compiler phases and the back end is uh the remaining boxes so the for simplicity in constructing the compiler we took we typically uh break this up into three main phases lexical analysis parsing also known as syntax analysis and semantic analysis lexical analysis reads one character at a time and groups the characters into the words of the language things like identifiers symbols like less than equal keywords um strings uh integer constants uh etc and uh while doing that uh any white space or comments are typically thrown away the part uh and these words in the language are represented as token objects the parser consumes the token objects recognizes the syntactic structure of the program where do expressions start and end where do statements start in the end and and then it represents uh the input program in in the form of a parse tree a tree data structure that mirrors the syntactic structure of the source code there are some things that a parser cannot do and so semantic analysis cleans up whatever the parser was not able to do things like uh type checking and uh in for doing this the three might get annotated and an uh an additional data structure is built called the symbol table which maps uh identifiers to the essentially to their type information the symbol table in might be used in in all other compiler phases it might be used in lexical analysis and parsing as well as in the compiler backend some compilers might only use the symbol table just for semantic analysis and um along the way it's possible that errors uh crop up they will be reported if there were no errors in the in the input file then we continue a compilation so then that intermediate representation those annotated parse trees are translated into um into a language independent and machine independent intermediate representation the intermediate code and then the compiler backend takes the intermediate code and generates code that includes all kinds of optimizations assembly instruction selection uh register allocation more optimizations and yet more optimizations for a production quality compiler by far the bulk of of a compiler is in the optimizations there can be optimizations earlier there can be optimizations during parsing during semantic analysis in intermediate code generation etc but the bulk of the optimizations are in the back end of the compiler this is also the general structure of uh of the compiler that we are going to build in this class uh project one is going to be lexical analysis project two is the parser project three is semantic analysis then uh project four is storage allocation deciding whether variables should reside in registers or in memory and if in memory in which memory location and then project five is intermediate code generation and project six is assembly instruction selection now for building for analyzing the source code there are theoretical tools that we can leverage so the set of all words in the language can forms irregular language the regular languages can be described using regular expressions and regular expressions can very easily be translated into a finite automata and a finite automaton would be a great tool to act as a lexicon lexical analyzer so uh what we will be doing is we'll take a compiler generation tool that uh takes uh a reg takes regular expressions as input and then generates uh a finite automaton as our lexical analyzer the syntax of a language can be conveniently described using a com can be conveniently described as a context-free language and again there are all kinds of different description mechanisms for that context-free grammars different flavors of those and they are tools that allow us to take um a con a context-free grammar description of a language and translate that into uh into a parse engine and that's a parser generator that's what we're gonna use for uh building our our parser in project tool similarly for uh semantic analysis that corresponds to context sensitive languages there are also description uh mechanisms like context context-sensitive grammars and parsing tools uh rewrite rule systems but context-sensitive grammars are too unwieldy to use in practice and the tools that would translate the context-sensitive grammars into into rewrite rule systems those are doubly exponential in the size of the input which means they are completely unpractical for production use so in practice what what we do for simatic analysis we simply write the semantic analysis by hand i should say some languages or actually quite a few languages have their lexical analyzers or parsers written by hand as well but uh the tools really are uh very useful especially for uh simple languages for some examples of the data structures suppose the input is the assignment x equals y times five then the tokens are a tiny little objects shown here using pairs of parentheses that contain an enumeration constant that spells out what kind of token is that and for some of the tokens the token also contains a value so for an identifier token we have as a value the name of the identifier for an integer literal token we have as a value the val the integer value and for the assignment multiply and semicolon tokens there are no values that are stored inside the token our token data structure also has position information away in the input file did that token occur which then is going to be used for error reporting but uh that's basically what the tokens look like the parse tree is a is a tree structure that mirrors the structure of the input program so here we have an assignment an assignment has a left hand side in the right hand side and correspondingly in the parse tree there is a left hand side in the right hand side of the assignment tree node and and recursively uh down the tree and uh during um during seminatic analysis or during type checking uh maybe we find out that x and y were declared as a floating point or real numbers so we might be adding annotations to the parse tree that those two identifiers are reals and um if y is a floating point number and the integer constant five is an integer we might be inserting a conversion node into the parse tree that converts the integer constant into a floating point constant in our compiler we are not doing that we'll we're not going to insert nodes into the tree but we are at um we are adding annotations to the tree and also along the way build uh the the symbol table is a separate data structure that keeps track that x is a real wise a real and so on actually in our compiler we don't have reals because floating point numbers are too complex to to process and uh don't contribute anything interesting to learn about compiler architecture now further on for intermediate code a type of intermediate code that in the past was very popular is called quadruples a quadruple is a sequence of very simple statements where each statement consists of four pieces of information two source registers an operation and a target register that means we need to explicitly assign uh that intermediate values of expressions twos two registers or two temporary names and then break this up into uh into such a sequence of quadruples what we are doing in our compiler is instead of a sequence of these tiny little statements we have a tree structure where um the edges in the tree correspond to that to those target uh identifiers conceptually uh very similar just um for modern languages um more convenient to process more memory intensive but more convenient to process and then finally uh assembly and uh for the purpose of this slide i'm not going to go into of this this introduction i'm not going to go into the details of what the assembly code looks like but these are the main data structures that we'll encounter in our compiler this is it for the introductory material in the next module we'll have a closer look at lexical analysis as an introduction to compilers let's have a look at the compiler architecture in general so if you look at a tool like say gcc the new c compiler it is not just a compiler it's it cons it drives multiple different tools so first of all the source code is not actual c source code instead it is c source code together with preprocessor macros like pound include pound define etc which could be called maybe skeletal source code and this is fed to the c preprocessor which then produces the actual source code now comes uh comes the compiler which translates the source code not to an object file not to an executable but to assembly text why because we already have an assembler in our system we already have uh other tools that can uh take it from the assembly text file all the way to an executable it is much easier to to generate assembly text than to generate an executable the assembler then assembles the code and produces a relocatable machine code relocatable means that the assembler produces object an object file for each individual source code but the object file is generated in the way that that it does not depend on the final memory location where in the executable the object uh that object file is going to reside so for example any branch instructions uh are going to be relative branch instructions relative to the current uh location uh or to the current program counter a jump plus a minus a certain number of bytes in um one direction or another instead of jumping to an addre to an actual uh fixed address uh in memory the reason for this is that then later in in the linker these different object files can be combined into a single executable and then eventually the loader loads that object file loads that executable into an operating systems process now if we zoom in a little bit what does the compiler look like the compiler reads read source code generates assembly and in between there's an intermediate representation and the compiler is written such that they can be multiple different frontends the front end is language specific the for the job of the front end is to analyze the source code and figure out what does it mean and the back end is machine specific that is it generates assembly code for the target architecture a compiler like gcc has a variety of different frontends and back ends gcc has on the order of 10 or 12 front ends c c plus plus objective c also known as objectionable c d yet another successor to see pascal fortran 77 fortran 90 um java and ada and probably another couple languages and the gcc has a back end for just about every processor that ever existed or is currently out there so in the order of uh 50 or so back-ends optimized for different processors and to make this work the intermediate representation needs to be language and architecture independent next if we look at this in a little bit more detail the compiler consists of multiple different phases uh if the analysis part is the first three or four boxes here in this vertical column of compiler phases and the back end is uh the remaining boxes so the for simplicity in constructing the compiler we took we typically uh break this up into three main phases lexical analysis parsing also known as syntax analysis and semantic analysis lexical analysis reads one character at a time and groups the characters into the words of the language things like identifiers symbols like less than equal keywords um strings uh integer constants uh etc and uh while doing that uh any white space or comments are typically thrown away the part uh and these words in the language are represented as token objects the parser consumes the token objects recognizes the syntactic structure of the program where do expressions start and end where do statements start in the end and and then it represents uh the input program in in the form of a parse tree a tree data structure that mirrors the syntactic structure of the source code there are some things that a parser cannot do and so semantic analysis cleans up whatever the parser was not able to do things like uh type checking and uh in for doing this the three might get annotated and an uh an additional data structure is built called the symbol table which maps uh identifiers to the essentially to their type information the symbol table in might be used in in all other compiler phases it might be used in lexical analysis and parsing as well as in the compiler backend some compilers might only use the symbol table just for semantic analysis and um along the way it's possible that errors uh crop up they will be reported if there were no errors in the in the input file then we continue a compilation so then that intermediate representation those annotated parse trees are translated into um into a language independent and machine independent intermediate representation the intermediate code and then the compiler backend takes the intermediate code and generates code that includes all kinds of optimizations assembly instruction selection uh register allocation more optimizations and yet more optimizations for a production quality compiler by far the bulk of of a compiler is in the optimizations there can be optimizations earlier there can be optimizations during parsing during semantic analysis in intermediate code generation etc but the bulk of the optimizations are in the back end of the compiler this is also the general structure of uh of the compiler that we are going to build in this class uh project one is going to be lexical analysis project two is the parser project three is semantic analysis then uh project four is storage allocation deciding whether variables should reside in registers or in memory and if in memory in which memory location and then project five is intermediate code generation and project six is assembly instruction selection now for building for analyzing the source code there are theoretical tools that we can leverage so the set of all words in the language can forms irregular language the regular languages can be described using regular expressions and regular expressions can very easily be translated into a finite automata and a finite automaton would be a great tool to act as a lexicon lexical analyzer so uh what we will be doing is we'll take a compiler generation tool that uh takes uh a reg takes regular expressions as input and then generates uh a finite automaton as our lexical analyzer the syntax of a language can be conveniently described using a com can be conveniently described as a context-free language and again there are all kinds of different description mechanisms for that context-free grammars different flavors of those and they are tools that allow us to take um a con a context-free grammar description of a language and translate that into uh into a parse engine and that's a parser generator that's what we're gonna use for uh building our our parser in project tool similarly for uh semantic analysis that corresponds to context sensitive languages there are also description uh mechanisms like context context-sensitive grammars and parsing tools uh rewrite rule systems but context-sensitive grammars are too unwieldy to use in practice and the tools that would translate the context-sensitive grammars into into rewrite rule systems those are doubly exponential in the size of the input which means they are completely unpractical for production use so in practice what what we do for simatic analysis we simply write the semantic analysis by hand i should say some languages or actually quite a few languages have their lexical analyzers or parsers written by hand as well but uh the tools really are uh very useful especially for uh simple languages for some examples of the data structures suppose the input is the assignment x equals y times five then the tokens are a tiny little objects shown here using pairs of parentheses that contain an enumeration constant that spells out what kind of token is that and for some of the tokens the token also contains a value so for an identifier token we have as a value the name of the identifier for an integer literal token we have as a value the val the integer value and for the assignment multiply and semicolon tokens there are no values that are stored inside the token our token data structure also has position information away in the input file did that token occur which then is going to be used for error reporting but uh that's basically what the tokens look like the parse tree is a is a tree structure that mirrors the structure of the input program so here we have an assignment an assignment has a left hand side in the right hand side and correspondingly in the parse tree there is a left hand side in the right hand side of the assignment tree node and and recursively uh down the tree and uh during um during seminatic analysis or during type checking uh maybe we find out that x and y were declared as a floating point or real numbers so we might be adding annotations to the parse tree that those two identifiers are reals and um if y is a floating point number and the integer constant five is an integer we might be inserting a conversion node into the parse tree that converts the integer constant into a floating point constant in our compiler we are not doing that we'll we're not going to insert nodes into the tree but we are at um we are adding annotations to the tree and also along the way build uh the the symbol table is a separate data structure that keeps track that x is a real wise a real and so on actually in our compiler we don't have reals because floating point numbers are too complex to to process and uh don't contribute anything interesting to learn about compiler architecture now further on for intermediate code a type of intermediate code that in the past was very popular is called quadruples a quadruple is a sequence of very simple statements where each statement consists of four pieces of information two source registers an operation and a target register that means we need to explicitly assign uh that intermediate values of expressions twos two registers or two temporary names and then break this up into uh into such a sequence of quadruples what we are doing in our compiler is instead of a sequence of these tiny little statements we have a tree structure where um the edges in the tree correspond to that to those target uh identifiers conceptually uh very similar just um for modern languages um more convenient to process more memory intensive but more convenient to process and then finally uh assembly and uh for the purpose of this slide i'm not going to go into of this this introduction i'm not going to go into the details of what the assembly code looks like but these are the main data structures that we'll encounter in our compiler this is it for the introductory material in the next module we'll have a closer look at lexical analysis\\n\" metadata={'title': 'Compilers Overview:\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100, separators=['.', '\\n'])\n",
    "\n",
    "# Function to prepare documents with metadata\n",
    "def prepare_documents_with_metadata(data, page_contents):\n",
    "    \"\"\"\n",
    "    Prepare a list of documents with metadata from a list of articles.\n",
    "    :param articles: A list of articles in JSON format.\n",
    "    :return: A list of Document objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    prepared_docs = []\n",
    "    title = \"\"\n",
    "    for line in data:\n",
    "        # Extract necessary fields from each article\n",
    "        if \":\" in line:\n",
    "            title = line\n",
    "        else:\n",
    "            content = line\n",
    "            # Create a Document object with metadata\n",
    "            doc = Document(page_content=content, metadata={\"title\": title})\n",
    "            prepared_docs.append(doc)\n",
    "    for page in page_contents:\n",
    "        doc = Document(page_content=page, metadata={\"title\": \"PDF Page\"})\n",
    "        prepared_docs.append(doc)\n",
    "    \n",
    "    return prepared_docs\n",
    "\n",
    "#json_data = json.loads(data_text)\n",
    "#print(len(json_data['rows']))\n",
    "docs = prepare_documents_with_metadata(data, page_contents) \n",
    "\n",
    "#docs = text_splitter.split_documents(docs)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'NFA to DFA Example: https://youtu.be/lbAY6uqq9fs\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining Semantic & Lexical Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the OpenAI embedding model to create vector embeddings for each chunk\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=my_openai_api_key)\n",
    "# Storing chunks along with their vector embeddings into a Chroma database\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "# Defining our semantic retriever, which will return the top-7 most semantically relevant chunks\n",
    "semantic_retriever = db.as_retriever(k=7)\n",
    "# Defining our lexical retriever, which uses the BM25 algorithm, to retrieve the top-7 most\n",
    "# lexically similar chunks\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, k=7)\n",
    "# Merge retrievers together into a single retriever, which will return up to 10 chunks\n",
    "merged_retriever = MergerRetriever(retrievers=[semantic_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reranker and Final Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are using Cohere Rerank as our compression algorithm\n",
    "compressor = CohereRerank( top_n=5)\n",
    "# We define a new retriever than first uses the base_retriever to retrieve documents and then the\n",
    "# base_compressor to filter them\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=merged_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM & Chain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "llm = ChatOpenAI(openai_api_key=my_openai_api_key, model=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining First Chain\n",
    "### This chain's job is to take a question and a chat history and create a version of the question that is contextualized with the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a prompt to contextualize the user's question\n",
    "\n",
    "contextualize_system_prompt = \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_chain = contextualize_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's Test the Contextualization Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating the RAG Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create our prompt\n",
    "rag_template = \"\"\"\n",
    "{context}\n",
    "You are an advisor for a student learning about compilers specifically lexical analysis and parsing. \n",
    "Use any information from the context to anwser the question. Site where you find the knowledge. If you can't find information from the document, use your own knowledge to answer the question.\n",
    "When needed use ascii art to help explain concepts and draw things like parse trees or tables.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating Main Chain\n",
    "### This chain needs to be able to dynamically determine if the question needs to be contextualized (which is not the case when there is no chat history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# itemgetter is built into Python and allows you to create a function that\n",
    "# returns the value of a key\n",
    "from operator import itemgetter\n",
    "# Create the chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | RunnablePassthrough().assign(context = itemgetter(\"question\") | compression_retriever)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Question Popup\")\n",
    "\n",
    "# Improved color scheme with an accent color\n",
    "dark_background = '#333333'  # Slightly lighter dark background for variety\n",
    "text_color = 'white'\n",
    "accent_color = '#4CAF50'  # A green accent color for interactive elements\n",
    "\n",
    "# Use modern fonts\n",
    "font_family = 'Arial'\n",
    "base_font_size = 12\n",
    "\n",
    "# Set the color scheme to dark mode with improved aesthetics\n",
    "window.configure(bg=dark_background)\n",
    "\n",
    "# Create a scrolled text widget for the chat history with improved visuals\n",
    "chat_history = scrolledtext.ScrolledText(window, width=70, height=10, bg=dark_background, fg=text_color, font=(font_family, base_font_size))\n",
    "chat_history.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)  # Added padding\n",
    "\n",
    "# Create a label and entry for the question with refined styling\n",
    "question_label = tk.Label(window, text=\"Enter your question:\", bg=dark_background, fg=text_color, font=(font_family, base_font_size))\n",
    "question_label.pack(pady=(10, 0))  # Added vertical padding for spacing\n",
    "question_entry = tk.Entry(window, width=100, bg=dark_background, fg=text_color, insertbackground=text_color)  # Ensure cursor is visible\n",
    "question_entry.pack(pady=(0, 10))  # Added vertical padding for spacing\n",
    "\n",
    "def get_question(event=None):\n",
    "    question = question_entry.get()\n",
    "    if question.strip() == \"\":\n",
    "        messagebox.showerror(\"Error\", \"Please enter a question.\")\n",
    "        return\n",
    "    chat_history.insert(tk.END, f\"\\nUser: {question}\\n\")\n",
    "    # Simulate response for demonstration\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    chat_history.insert(tk.END, f\"AI: {response}\\n\")\n",
    "    chat_history.see(tk.END)\n",
    "    question_entry.delete(0, tk.END)\n",
    "\n",
    "# Create a button to submit the question with improved design\n",
    "submit_button = tk.Button(window, text=\"Submit\", command=get_question, bg=accent_color, fg='white', font=(font_family, base_font_size), bd=0, padx=10, pady=5)\n",
    "submit_button.pack()\n",
    "\n",
    "# Bind the Enter key to submit the question\n",
    "window.bind('<Return>', get_question)\n",
    "\n",
    "# Run the main event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
