{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#must have python 3.10 installed\n",
    "!pip install pandas langchain-openai langchain-core langchain-community langchain pypdf chromadb rank-bm25 cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts.chat import MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_openai_api_key = \"sk-proj-tNP7Vnu7BbwUEFCJ6HjDT3BlbkFJMSochvTWayhj0tuxLKsp\"\n",
    "os.environ[\"COHERE_API_KEY\"] = \"1JFzujZ48tZDZj2Uk3QTYdebYFiCNmczn8hxvCgu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Loading and Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"temp.txt\"\n",
    "data = []\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        if line != '\\n':\n",
    "            data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.pdf import PyPDFLoader\n",
    "\n",
    "file_paths = [\n",
    "    \"5e/core/D&D 5e - DM Guide.pdf\",\n",
    "    \"5e/core/D&D 5E - Monster Manual.pdf\",\n",
    "    \"5e/core/D&D 5E - Player's Handbook.pdf\",\n",
    "    \"5e/expansions/D&D 5E - Mordenkainen's Tome of Foes.pdf\",\n",
    "    \"5e/expansions/D&D 5e - Tashaâ€™s Cauldron of Everything.pdf\",\n",
    "    \"5e/expansions/D&D 5E - Volo's Guide to Monsters.pdf\",\n",
    "    \"5e/expansions/D&D 5E - Xanathar's Guide to Everything.pdf\",\n",
    "    \"homebrew/Heiraxia.pdf\"\n",
    "]\n",
    "page_contents = []\n",
    "for file_path in file_paths:\n",
    "    pdf_loader = PyPDFLoader(file_path)\n",
    "    docs = pdf_loader.load()\n",
    "    page_contents += [doc.page_content for doc in docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100, separators=['.', '\\n'])\n",
    "\n",
    "# Function to prepare documents with metadata\n",
    "def prepare_documents_with_metadata(data, page_contents):\n",
    "    \"\"\"\n",
    "    Prepare a list of documents with metadata from a list of articles.\n",
    "    :param articles: A list of articles in JSON format.\n",
    "    :return: A list of Document objects.\n",
    "    \"\"\"\n",
    "    \n",
    "    prepared_docs = []\n",
    "    title = \"\"\n",
    "    for line in data:\n",
    "        # Extract necessary fields from each article\n",
    "        if \":\" in line:\n",
    "            title = line\n",
    "        else:\n",
    "            content = line\n",
    "            # Create a Document object with metadata\n",
    "            doc = Document(page_content=content, metadata={\"title\": title})\n",
    "            prepared_docs.append(doc)\n",
    "    for page in page_contents:\n",
    "        doc = Document(page_content=page, metadata={\"title\": \"PDF Page\"})\n",
    "        prepared_docs.append(doc)\n",
    "    \n",
    "    return prepared_docs\n",
    "\n",
    "\n",
    "docs = prepare_documents_with_metadata(data, page_contents) \n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[10].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining Semantic & Lexical Retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using the OpenAI embedding model to create vector embeddings for each chunk\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=my_openai_api_key)\n",
    "# Storing chunks along with their vector embeddings into a Chroma database\n",
    "db = Chroma.from_documents(docs, embeddings)\n",
    "# Defining our semantic retriever, which will return the top-7 most semantically relevant chunks\n",
    "semantic_retriever = db.as_retriever(k=7)\n",
    "# Defining our lexical retriever, which uses the BM25 algorithm, to retrieve the top-7 most\n",
    "# lexically similar chunks\n",
    "bm25_retriever = BM25Retriever.from_documents(docs, k=7)\n",
    "# Merge retrievers together into a single retriever, which will return up to 10 chunks\n",
    "merged_retriever = MergerRetriever(retrievers=[semantic_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Reranker and Final Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We are using Cohere Rerank as our compression algorithm\n",
    "compressor = CohereRerank( top_n=5)\n",
    "# We define a new retriever than first uses the base_retriever to retrieve documents and then the\n",
    "# base_compressor to filter them\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=merged_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# LLM & Chain Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ChatGPT\n",
    "llm = ChatOpenAI(openai_api_key=my_openai_api_key, model=\"gpt-3.5-turbo\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defining First Chain\n",
    "### This chain's job is to take a question and a chat history and create a version of the question that is contextualized with the chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a prompt to contextualize the user's question\n",
    "\n",
    "contextualize_system_prompt = \"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "contextualize_chain = contextualize_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let's Test the Contextualization Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating the RAG Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create our prompt\n",
    "rag_template = \"\"\"\n",
    "{context}\n",
    "You are a search engine for a Dungeons and Dragons 5th edition Dungeon Master who is accessing rules, stats, and details\n",
    "from the rulebooks as well as homebrew content. \n",
    "Use any information from the context to anwser the question. Site where you find the knowledge. \n",
    "If you can't find information from the documents, use your own knowledge to answer the question.\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(rag_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Creating Main Chain\n",
    "### This chain needs to be able to dynamically determine if the question needs to be contextualized (which is not the case when there is no chat history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# itemgetter is built into Python and allows you to create a function that\n",
    "# returns the value of a key\n",
    "from operator import itemgetter\n",
    "# Create the chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough()\n",
    "    | RunnablePassthrough().assign(context = itemgetter(\"question\") | compression_retriever)\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import scrolledtext\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Tower of Inquiry\")\n",
    "\n",
    "# Improved color scheme with an accent color\n",
    "paper_background = '#f8f8f8'  # \n",
    "text_color = 'black'\n",
    "accent_color = '#000000'  # A maroon accent color for interactive elements\n",
    "\n",
    "# Use modern fonts\n",
    "font_family = 'Baskerville'\n",
    "base_font_size = 22\n",
    "font_style = 'italic'\n",
    "\n",
    "# Set the color scheme to dark mode with improved aesthetics\n",
    "window.configure(bg=paper_background)\n",
    "\n",
    "# Create a scrolled text widget for the chat history with improved visuals\n",
    "chat_history = scrolledtext.ScrolledText(window, width=70, height=10, bg=paper_background, fg=text_color, font=(font_family, base_font_size, font_style))\n",
    "chat_history.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)  # Added padding\n",
    "\n",
    "# Create a label and entry for the question with refined styling\n",
    "question_label = tk.Label(window, text=\"Enter your inquiry:\", bg=paper_background, fg=text_color, font=(font_family, base_font_size))\n",
    "question_label.pack(pady=(10, 10))  # Added vertical padding for spacing\n",
    "question_entry = tk.Entry(window, width=100, bg=paper_background, fg=text_color, insertbackground=text_color)  # Ensure cursor is visible\n",
    "question_entry.pack(pady=(10, 10))  # Added vertical padding for spacing\n",
    "\n",
    "def get_question(event=None):\n",
    "    question = question_entry.get()\n",
    "    if question.strip() == \"\":\n",
    "        messagebox.showerror(\"Error\", \"Please enter an inquiry.\")\n",
    "        return\n",
    "    chat_history.insert(tk.END, f\"\\nUser: {question}\\n\\n\")\n",
    "    # Simulate response for demonstration\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    chat_history.insert(tk.END, f\"Lorekeeper: {response}\\n\")\n",
    "    chat_history.see(tk.END)\n",
    "    question_entry.delete(0, tk.END)\n",
    "\n",
    "# Create a button to submit the question with improved design\n",
    "submit_button = tk.Button(window, text=\"Inquire\", command=get_question, background=accent_color, foreground=text_color, font=(font_family, base_font_size), bd=0, padx=10, pady=5)\n",
    "submit_button.pack()\n",
    "\n",
    "# Bind the Enter key to submit the question\n",
    "window.bind('<Return>', get_question)\n",
    "\n",
    "# Run the main event loop\n",
    "window.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
