Compilers Overview:
as an introduction to compilers let's have a look at the compiler architecture in general so if you look at a tool like say gcc the new c compiler it is not just a compiler it's it cons it drives multiple different tools so first of all the source code is not actual c source code instead it is c source code together with preprocessor macros like pound include pound define etc which could be called maybe skeletal source code and this is fed to the c preprocessor which then produces the actual source code now comes uh comes the compiler which translates the source code not to an object file not to an executable but to assembly text why because we already have an assembler in our system we already have uh other tools that can uh take it from the assembly text file all the way to an executable it is much easier to to generate assembly text than to generate an executable the assembler then assembles the code and produces a relocatable machine code relocatable means that the assembler produces object an object file for each individual source code but the object file is generated in the way that that it does not depend on the final memory location where in the executable the object uh that object file is going to reside so for example any branch instructions uh are going to be relative branch instructions relative to the current uh location uh or to the current program counter a jump plus a minus a certain number of bytes in um one direction or another instead of jumping to an addre to an actual uh fixed address uh in memory the reason for this is that then later in in the linker these different object files can be combined into a single executable and then eventually the loader loads that object file loads that executable into an operating systems process now if we zoom in a little bit what does the compiler look like the compiler reads read source code generates assembly and in between there's an intermediate representation and the compiler is written such that they can be multiple different frontends the front end is language specific the for the job of the front end is to analyze the source code and figure out what does it mean and the back end is machine specific that is it generates assembly code for the target architecture a compiler like gcc has a variety of different frontends and back ends gcc has on the order of 10 or 12 front ends c c plus plus objective c also known as objectionable c d yet another successor to see pascal fortran 77 fortran 90 um java and ada and probably another couple languages and the gcc has a back end for just about every processor that ever existed or is currently out there so in the order of uh 50 or so back-ends optimized for different processors and to make this work the intermediate representation needs to be language and architecture independent next if we look at this in a little bit more detail the compiler consists of multiple different phases uh if the analysis part is the first three or four boxes here in this vertical column of compiler phases and the back end is uh the remaining boxes so the for simplicity in constructing the compiler we took we typically uh break this up into three main phases lexical analysis parsing also known as syntax analysis and semantic analysis lexical analysis reads one character at a time and groups the characters into the words of the language things like identifiers symbols like less than equal keywords um strings uh integer constants uh etc and uh while doing that uh any white space or comments are typically thrown away the part uh and these words in the language are represented as token objects the parser consumes the token objects recognizes the syntactic structure of the program where do expressions start and end where do statements start in the end and and then it represents uh the input program in in the form of a parse tree a tree data structure that mirrors the syntactic structure of the source code there are some things that a parser cannot do and so semantic analysis cleans up whatever the parser was not able to do things like uh type checking and uh in for doing this the three might get annotated and an uh an additional data structure is built called the symbol table which maps uh identifiers to the essentially to their type information the symbol table in might be used in in all other compiler phases it might be used in lexical analysis and parsing as well as in the compiler backend some compilers might only use the symbol table just for semantic analysis and um along the way it's possible that errors uh crop up they will be reported if there were no errors in the in the input file then we continue a compilation so then that intermediate representation those annotated parse trees are translated into um into a language independent and machine independent intermediate representation the intermediate code and then the compiler backend takes the intermediate code and generates code that includes all kinds of optimizations assembly instruction selection uh register allocation more optimizations and yet more optimizations for a production quality compiler by far the bulk of of a compiler is in the optimizations there can be optimizations earlier there can be optimizations during parsing during semantic analysis in intermediate code generation etc but the bulk of the optimizations are in the back end of the compiler this is also the general structure of uh of the compiler that we are going to build in this class uh project one is going to be lexical analysis project two is the parser project three is semantic analysis then uh project four is storage allocation deciding whether variables should reside in registers or in memory and if in memory in which memory location and then project five is intermediate code generation and project six is assembly instruction selection now for building for analyzing the source code there are theoretical tools that we can leverage so the set of all words in the language can forms irregular language the regular languages can be described using regular expressions and regular expressions can very easily be translated into a finite automata and a finite automaton would be a great tool to act as a lexicon lexical analyzer so uh what we will be doing is we'll take a compiler generation tool that uh takes uh a reg takes regular expressions as input and then generates uh a finite automaton as our lexical analyzer the syntax of a language can be conveniently described using a com can be conveniently described as a context-free language and again there are all kinds of different description mechanisms for that context-free grammars different flavors of those and they are tools that allow us to take um a con a context-free grammar description of a language and translate that into uh into a parse engine and that's a parser generator that's what we're gonna use for uh building our our parser in project tool similarly for uh semantic analysis that corresponds to context sensitive languages there are also description uh mechanisms like context context-sensitive grammars and parsing tools uh rewrite rule systems but context-sensitive grammars are too unwieldy to use in practice and the tools that would translate the context-sensitive grammars into into rewrite rule systems those are doubly exponential in the size of the input which means they are completely unpractical for production use so in practice what what we do for simatic analysis we simply write the semantic analysis by hand i should say some languages or actually quite a few languages have their lexical analyzers or parsers written by hand as well but uh the tools really are uh very useful especially for uh simple languages for some examples of the data structures suppose the input is the assignment x equals y times five then the tokens are a tiny little objects shown here using pairs of parentheses that contain an enumeration constant that spells out what kind of token is that and for some of the tokens the token also contains a value so for an identifier token we have as a value the name of the identifier for an integer literal token we have as a value the val the integer value and for the assignment multiply and semicolon tokens there are no values that are stored inside the token our token data structure also has position information away in the input file did that token occur which then is going to be used for error reporting but uh that's basically what the tokens look like the parse tree is a is a tree structure that mirrors the structure of the input program so here we have an assignment an assignment has a left hand side in the right hand side and correspondingly in the parse tree there is a left hand side in the right hand side of the assignment tree node and and recursively uh down the tree and uh during um during seminatic analysis or during type checking uh maybe we find out that x and y were declared as a floating point or real numbers so we might be adding annotations to the parse tree that those two identifiers are reals and um if y is a floating point number and the integer constant five is an integer we might be inserting a conversion node into the parse tree that converts the integer constant into a floating point constant in our compiler we are not doing that we'll we're not going to insert nodes into the tree but we are at um we are adding annotations to the tree and also along the way build uh the the symbol table is a separate data structure that keeps track that x is a real wise a real and so on actually in our compiler we don't have reals because floating point numbers are too complex to to process and uh don't contribute anything interesting to learn about compiler architecture now further on for intermediate code a type of intermediate code that in the past was very popular is called quadruples a quadruple is a sequence of very simple statements where each statement consists of four pieces of information two source registers an operation and a target register that means we need to explicitly assign uh that intermediate values of expressions twos two registers or two temporary names and then break this up into uh into such a sequence of quadruples what we are doing in our compiler is instead of a sequence of these tiny little statements we have a tree structure where um the edges in the tree correspond to that to those target uh identifiers conceptually uh very similar just um for modern languages um more convenient to process more memory intensive but more convenient to process and then finally uh assembly and uh for the purpose of this slide i'm not going to go into of this this introduction i'm not going to go into the details of what the assembly code looks like but these are the main data structures that we'll encounter in our compiler this is it for the introductory material in the next module we'll have a closer look at lexical analysis as an introduction to compilers let's have a look at the compiler architecture in general so if you look at a tool like say gcc the new c compiler it is not just a compiler it's it cons it drives multiple different tools so first of all the source code is not actual c source code instead it is c source code together with preprocessor macros like pound include pound define etc which could be called maybe skeletal source code and this is fed to the c preprocessor which then produces the actual source code now comes uh comes the compiler which translates the source code not to an object file not to an executable but to assembly text why because we already have an assembler in our system we already have uh other tools that can uh take it from the assembly text file all the way to an executable it is much easier to to generate assembly text than to generate an executable the assembler then assembles the code and produces a relocatable machine code relocatable means that the assembler produces object an object file for each individual source code but the object file is generated in the way that that it does not depend on the final memory location where in the executable the object uh that object file is going to reside so for example any branch instructions uh are going to be relative branch instructions relative to the current uh location uh or to the current program counter a jump plus a minus a certain number of bytes in um one direction or another instead of jumping to an addre to an actual uh fixed address uh in memory the reason for this is that then later in in the linker these different object files can be combined into a single executable and then eventually the loader loads that object file loads that executable into an operating systems process now if we zoom in a little bit what does the compiler look like the compiler reads read source code generates assembly and in between there's an intermediate representation and the compiler is written such that they can be multiple different frontends the front end is language specific the for the job of the front end is to analyze the source code and figure out what does it mean and the back end is machine specific that is it generates assembly code for the target architecture a compiler like gcc has a variety of different frontends and back ends gcc has on the order of 10 or 12 front ends c c plus plus objective c also known as objectionable c d yet another successor to see pascal fortran 77 fortran 90 um java and ada and probably another couple languages and the gcc has a back end for just about every processor that ever existed or is currently out there so in the order of uh 50 or so back-ends optimized for different processors and to make this work the intermediate representation needs to be language and architecture independent next if we look at this in a little bit more detail the compiler consists of multiple different phases uh if the analysis part is the first three or four boxes here in this vertical column of compiler phases and the back end is uh the remaining boxes so the for simplicity in constructing the compiler we took we typically uh break this up into three main phases lexical analysis parsing also known as syntax analysis and semantic analysis lexical analysis reads one character at a time and groups the characters into the words of the language things like identifiers symbols like less than equal keywords um strings uh integer constants uh etc and uh while doing that uh any white space or comments are typically thrown away the part uh and these words in the language are represented as token objects the parser consumes the token objects recognizes the syntactic structure of the program where do expressions start and end where do statements start in the end and and then it represents uh the input program in in the form of a parse tree a tree data structure that mirrors the syntactic structure of the source code there are some things that a parser cannot do and so semantic analysis cleans up whatever the parser was not able to do things like uh type checking and uh in for doing this the three might get annotated and an uh an additional data structure is built called the symbol table which maps uh identifiers to the essentially to their type information the symbol table in might be used in in all other compiler phases it might be used in lexical analysis and parsing as well as in the compiler backend some compilers might only use the symbol table just for semantic analysis and um along the way it's possible that errors uh crop up they will be reported if there were no errors in the in the input file then we continue a compilation so then that intermediate representation those annotated parse trees are translated into um into a language independent and machine independent intermediate representation the intermediate code and then the compiler backend takes the intermediate code and generates code that includes all kinds of optimizations assembly instruction selection uh register allocation more optimizations and yet more optimizations for a production quality compiler by far the bulk of of a compiler is in the optimizations there can be optimizations earlier there can be optimizations during parsing during semantic analysis in intermediate code generation etc but the bulk of the optimizations are in the back end of the compiler this is also the general structure of uh of the compiler that we are going to build in this class uh project one is going to be lexical analysis project two is the parser project three is semantic analysis then uh project four is storage allocation deciding whether variables should reside in registers or in memory and if in memory in which memory location and then project five is intermediate code generation and project six is assembly instruction selection now for building for analyzing the source code there are theoretical tools that we can leverage so the set of all words in the language can forms irregular language the regular languages can be described using regular expressions and regular expressions can very easily be translated into a finite automata and a finite automaton would be a great tool to act as a lexicon lexical analyzer so uh what we will be doing is we'll take a compiler generation tool that uh takes uh a reg takes regular expressions as input and then generates uh a finite automaton as our lexical analyzer the syntax of a language can be conveniently described using a com can be conveniently described as a context-free language and again there are all kinds of different description mechanisms for that context-free grammars different flavors of those and they are tools that allow us to take um a con a context-free grammar description of a language and translate that into uh into a parse engine and that's a parser generator that's what we're gonna use for uh building our our parser in project tool similarly for uh semantic analysis that corresponds to context sensitive languages there are also description uh mechanisms like context context-sensitive grammars and parsing tools uh rewrite rule systems but context-sensitive grammars are too unwieldy to use in practice and the tools that would translate the context-sensitive grammars into into rewrite rule systems those are doubly exponential in the size of the input which means they are completely unpractical for production use so in practice what what we do for simatic analysis we simply write the semantic analysis by hand i should say some languages or actually quite a few languages have their lexical analyzers or parsers written by hand as well but uh the tools really are uh very useful especially for uh simple languages for some examples of the data structures suppose the input is the assignment x equals y times five then the tokens are a tiny little objects shown here using pairs of parentheses that contain an enumeration constant that spells out what kind of token is that and for some of the tokens the token also contains a value so for an identifier token we have as a value the name of the identifier for an integer literal token we have as a value the val the integer value and for the assignment multiply and semicolon tokens there are no values that are stored inside the token our token data structure also has position information away in the input file did that token occur which then is going to be used for error reporting but uh that's basically what the tokens look like the parse tree is a is a tree structure that mirrors the structure of the input program so here we have an assignment an assignment has a left hand side in the right hand side and correspondingly in the parse tree there is a left hand side in the right hand side of the assignment tree node and and recursively uh down the tree and uh during um during seminatic analysis or during type checking uh maybe we find out that x and y were declared as a floating point or real numbers so we might be adding annotations to the parse tree that those two identifiers are reals and um if y is a floating point number and the integer constant five is an integer we might be inserting a conversion node into the parse tree that converts the integer constant into a floating point constant in our compiler we are not doing that we'll we're not going to insert nodes into the tree but we are at um we are adding annotations to the tree and also along the way build uh the the symbol table is a separate data structure that keeps track that x is a real wise a real and so on actually in our compiler we don't have reals because floating point numbers are too complex to to process and uh don't contribute anything interesting to learn about compiler architecture now further on for intermediate code a type of intermediate code that in the past was very popular is called quadruples a quadruple is a sequence of very simple statements where each statement consists of four pieces of information two source registers an operation and a target register that means we need to explicitly assign uh that intermediate values of expressions twos two registers or two temporary names and then break this up into uh into such a sequence of quadruples what we are doing in our compiler is instead of a sequence of these tiny little statements we have a tree structure where um the edges in the tree correspond to that to those target uh identifiers conceptually uh very similar just um for modern languages um more convenient to process more memory intensive but more convenient to process and then finally uh assembly and uh for the purpose of this slide i'm not going to go into of this this introduction i'm not going to go into the details of what the assembly code looks like but these are the main data structures that we'll encounter in our compiler this is it for the introductory material in the next module we'll have a closer look at lexical analysis




Lexical Analysis: 
the first part of our compiler is lexical analysis this material is covered in our textbook in chapter 2. so what does lexical analysis do in a typical compiler the lexical analyzer eliminates white space and comments groups the individual characters into tokens and while doing so speed is important now i said in a typical compiler there are of course differences so for example some programming languages like python or haskell have a semantic semantic meaning attached to the indentation of code so uh for example an if block is indented relative to the if statement itself and so therefore the lexical analyzer could not simply throw away all white space instead it would need to count the white space characters and uh translate them into an increase in indentation versus a decrease in annotation similarly for comments the lexical analyzer for javadoc which processes javadoc together with the comments and generates html from that that lexical analyzer obviously cannot throw away comments and uh similarly there are some languages where uh where the compilers have recognize uh special especially formatted uh comments uh that can contain for example optimization flags for the compiler or other directives for the compiler why is speed important because the lexical analyzer needs to touch every single character in the input file for example if you have a very long identifier with say 30 characters the lexical analyzer needs to touch every single character which is a memory access at the minimum for every single character while the parser or other parts of the compiler simply can hand around and identify an object instead of having to deal with the individual characters and furthermore the the lexical analyzer needs to read from a file and if every single character is read from the file that of course would become way too expensive so it's necessary to have buffered input so that for example an entire line is read into into memory and then individual characters are accessed in memory instead of going to uh to the input file for every single character because of the importance of speed in the past some compilers had handwritten lexical analyzers because of speeds for example gcc used to have the the older versions of gcc which i worked on for example for my uh dissertation research had a handwritten lexical analyzer because of speed but then used the parser generator for writing the parser recent versions of gcc had translated the parser into a a handwritten parser i don't know why my guess is because of readability and maintainability more so than uh than speed or convenience some terminology that's important for lexical analysis elixim is the string of characters that are matched when when reading a word in the language so for example for for the integer constant 42. the lexeme is the string of characters containing the ascii character for two followed by the ask the ascii character for four followed by the ascii character for two or depending on the programming language and the character encoding unicode characters instead of ascii characters the token is a data structure that represents the lexeme for further processing the token is the data structure with which the lexical information is communicated between the lexical analyzer and the parser and in a token object we essentially have an identifier or an enumeration constant that says what type of token is that plus uh for some tokens the value that comes with the token so as uh examples we had typical examples of tokens or corresponding like themes in a programming language we have all kinds of values or constants integer constants floating point constants boolean constants character and string constants we can have identifiers keyboards like if and while as well as as individual character symbols or symbols consisting of multiple characters like less than less equal and so on the that the tokens that would be generated from that are then a tiny little objects so for example uh say the input is x equals y times five with a few spaces in between the semicolon at the end the tokens that would be produced would be individual objects identified here using those pairs of parentheses first an identifier token which happens to have the value x that might be the string that was read for the identifier or the lex theme for the identifier then an assignment token the identifier y multiplication token the integer constant 5 where the value might be for example an int or a childhood language integer object and finally a semicolon


Regular Expression: 
for a handwritten lexical analyzer we would simply read a character from the buffered input file check what it is is it the start of an identifier the start of an integer a constant etc and then base based on that uh read additional characters until we find the end of our lexine then uh construct the token and return that to the parses of fairly straightforward code structure for this compiler we are using a lexical analyzer generator in the way this works is we describe our lexemes as regular expressions so we need to see what the regular expressions are and then those regular expressions would first be translated into a non-deterministic kind of automaton then the non-deterministic kind of automaton is translated into a deterministic kind of automaton and then the dfa is implemented as a table driven automata and that entire process from regular expressions to generating this table derivative automaton is done by the lexical analyzer generator for us so what are irregular expressions a regular expression could be an individual symbol as in an individual character say the character a regular expressions can be built using alternation the alternation operator is a vertical bar for example a or b that means we expect to see either the character a or the character b in the input concatenation a followed by b that means we expect to see the character a followed by the character b in the input repetition uh this the star operator also known as the clinic closure after some logician whose last name was clini uh says that whatever comes before the star is repeated zero or more times and then larger regular expressions can be built using parentheses or we can have an empty regular expression when uh writing regular expressions by hand the empty regular expression is often denoted by the uh greek character epsilon for uh for input for a a tool that works with regular expressions obviously we would just have an empty string we don't usually have uh greek characters as input for tools like lexical analyzer generators for some examples an integer constant could be written as the the following regular expression we have either the individual digit zero or we have a first digit which is one or two or three or four five six seven or eight or nine followed by a zero or more occurrences of inde of additional digits which are then zero through nine so uh this specifies integer constants without leading zeros or for identifiers we would we might have a first identifier which might be a lowercase character a to z followed by a lowercase or uppercase characters a to z and um in digits 0 to 9 or maybe also underscores or dollar signs so how regular express how identifies are defined obviously that depends on the programming language these would be um that would be the syntax for example for predicates and prologue but some languages have meaningful differences between lowercase and uppercase characters some languages even allow unicode characters in identifiers such as java or they might allow underscores as its first characters as well so there's all kinds of variations for how identifiers are defined in different programming languages but no matter how it is specified it's relatively straightforward to write it down as a regular expression now the regular expressions on the previous slide were somewhat awkward to write and especially that dot dot dot that was actually an abbreviation on the slide that really should have said a or b or c or d etc all the way to a z which of course would be too much uh to type so uh tools that work with regular expressions usually have all kinds of abbreviations first of all concatenation binds tighter than alternation so a b or c is a shorthand for open paren a b closing paren or c this is similar to arithmetic think of concatenation being the equivalent of multiplication which in arithmetic is also uh often written as concatenation at least in in math instead of the star in programming languages and the vertical bar corresponds to uh to addition and so similar as in um in arithmetic multiplication points tighter than uh than addition and here concatenation binds tighter than or in fact if you look at the um at the properties of these operators the algebraic properties of concatenation and or they're the exact same as the algebraic properties of multiplication and uh and plus in both of them i believe or uh abstract in abstract algebra would be rings other abbreviations um a set of characters in square brackets is an abbreviation for open like left square bracket abcd right square bracket is an abbreviation for a or b or c or d and if you have character ranges like a hyphen z in square bracket that's an abbreviation for enumerating all the 26 characters from from a to z if the very first character inside square brackets is a tilde sign then this means any character other than the characters that are listed this works uh with multiple characters for x as well as with um with character ranges like hyphens then uh x question mark or an arbitrary regular expression followed by a question mark is an abbreviation for this regular expression or epsilon that means it's optional and the post-fix operator plus means the previous regular expression is uh is repeated it's repeated one or more times as opposed to the star meaning zero more times and if some of those operators like question mark plus star etc should be used for representing individual input characters say you have you want to recognize the plus operator in the input then you can either enclose it in double quotes or you can put a backslash as an escape character behind and finally the dot by itself is a shorthand for um for the character class anything but the new line character that means um the dot stands for all ascii characters or all unicode characters whatever character class we're using except for the new line character


JLEX: https://youtu.be/WSC5Oo6fwrc
so how do we work with uh jlx for writing lexical analyzer for our tiger compiler for uh project one we we only have to deal with five files for this project there is a class error message in the error message package which keeps track of line numbers and prints error messages in the parse package there's a one-line interface called lexa which is the interface for our lexical analyzer class as well as the main program which for testing purposes it just uh reads uh gets one token after another from the lexical analyzer and then prints it back out the uh enumeration constants for uh for the token objects or in simba java this file is generated by java cup when later when we write the parser we will have declarations for all the tokens in our language in the source file for the parser generator and from that the parser generator generates this filezim.java so you should not modify that obviously if it's generated code and finally we have a tiger.lex which is the source file for jlex and jlex takes this file and translates it into a java code it means for building um a workable uh a workable program we first need to run jlex and then compile all the java code the the jlex source file consists of three parts those three parts are separated by a percent percent uh this structure of the j-like source file that that was inherited from lex the the lexical analyzer generator that was developed uh together with uh with unix in the early 1970s and that that targeted sea so it built a lexical analyzer written in c and compatible with linking against c and uh here j lex is just a uh essentially re-implementation of flags for java so the first part of the file is a java code that gets copied from the jlk source file to the beginning of of the generated of the generated java file then after the first percent percent between percent uh in the second section of the source file between percent open curly brace and percent closing curly brace there's more java code that gets copied into the generated class with and then there are a variety of different declarations so for example percent function declares in the name of the method that that is the main method for calling the lexical analyzer and there are multiple of these declarations for all kinds of different purposes then we can have regular expression macros so in this case the macro digits is defined to be one or more characters from the character class zero to nine and then we have our second percent percent and then the whole rest of the file is written in two columns where the first column is a regular expression and the second column is a piece of java code in curly braces the strategy that jlex uses for processing the input there are a couple important um important strategies first the longest match if there are multiple rules that that have the same prefix or multiple regular expressions that match the same prefix but one regular expression matches a large number of characters if if you happen to have that large number of characters then that regular expression went so if you have a regular expression that matches a less than sign a regular expression that matches less than equal and we actually have less equal in the input than the regular expression that matches less than equal wins out and we actually uh recognize less equal as a single lexing instead of as two lex seems less than followed by an equal rule priority if there are multiple regular expressions that can match the same input the same sets of characters then whichever regular expression uh comes first in the input file in the jlx file that regular expression wins out so for example if we have a regular expression for the keyword if and then a regular expression for identifiers here in curly braces that uses the ident macro which would be a regular expression macro and suppose the input is the keyword if then both of those regular expressions would match that keyword if but because the keyword was the rule for the keyboard was listed first that's the one that went out and we returned the keyword token instead of the identifier token to to the parser or to the main file in our case now for understanding how the lexical analyzer works in detail that we need a we need to look a little bit about what happens one character at the time this is an example uh from the textbook that's sort of contrived so the idea here is in this language a comment starts with two minus signs and goes all the way to the end of the line that part is not contrived there are languages that have comments like this i believe ada starts comments with two minus signs but then the contrived partisan that that in that particular language a comment cannot contain other minus signs so how would uh era how would the parser process this the uh the lexical how about the lexical analyzer processes the first time we call the lexical analyzer we start uh in the green position that is we start uh reading at the very beginning of the line we read the keyword if as we read one character at a time we advance the red pointer so initially all three pointers started at the very beginning as we read one character at a time we we keep advancing the red pointer and the blue pointer at the top points to the last position where we had a final state in our regulates in our automaton uh we'll we'll get to the details for how automata work uh a little bit later but wherever we can recognize uh a token so after the i we can recognize the identifier i so it be we advance the blue pointer then we read the the character f we advance the red pointer that could be the keyword if so we advance the blue pointer then we we advance the red point of this c the space character now uh there is no way that uh the string i f space could be part of any uh of any legal exim so we back up to the last point where we had the blue pointer and that's what we return that so we return the if uh the if token to the parser with the next call to the lexical analyzer all three pointers start after the f uh we advance the red pointer by one to uh to read the single space we advance the blue pointer that's the final state that's the regular expression that recognizes a state in order to discard uh that character and because uh for the for re for reading the space there is no java code attached to it that means we automatically restart the lexical analyzer and now comes the interesting part we uh we start all three pointers at the beginning of the minus minus we see the first minus we had using the right pointer we advanced the blue pointer because that could be the operator minus then we advance the the red pointer we see the second minus that could be the start of a comment but it's not a finished comment yet because the comet only finishes with the end of the line so we keep advancing the red pointer until until after the knot and until we get the first minus sign between not and a and at that point we recognize this is not a legal uh comment anymore so therefore we backtrack all the way to after the first minus sign we return the minus operator and then we restart the lexical analyzer so we end up having two minus operators and identify not minus operator identified minus operator and finally another identifier for comment so the point uh here is that potentially with this red pointer the lexical analyzer can read quite far ahead until it finds out this does not leads to illegal uh lexeme anymore and then we backtrack all the way to the place where we saw the the last legal election in this case uh for comments the last video like theme is just the minus operator and then we backtrack all the way to that


Start State: https://youtu.be/9CNbf_udUtU
sometimes we want to recognize like seems using not not using a single regular expression but as a combination of multiple regular expressions and this can be done with start states in jailx so start states allow breaking the recognition of the token ortholexine into multiple regular expressions and one example is uh if our language allows nested comments which tiger does uh nested comments is a or a language construct that that cannot be described with a single regular expression so it would be necessary to to use a mechanism other than regular expressions for recognizing comments and would start with start states we can add a little code fragments to those regular expressions so that we can recognize nested comments another example is if you want to associate additional processing additional code with the recognition of complex lexemes for example in string constants wherever we have escape character sequences like backslash n we would like to replace those escape character sequences with the corresponding character that they represent like uh the new line character and again that is something that can be conveniently done with start states as an example for what start states look like syntactically in the second part of the input file we have a declaration percent state that lists the start state and if the if you have multiple start states then you can uh list them all with with a single percent state separate with the state separated by commas and then in the third section of the file now any states any regular expressions that you had previously before you before you added the start states to your input file those now need to be prefixed by all caps y by initial inside a triangular bracket so for example the the regular expression for the keyword if now it needs to say uh less than sign y by initial greater than sign and then we have the regular expression for the keyword if and then in the code fragment attached to it we would have we would return the if token to the parser now uh for this example comments start with open paren star and end with a closing parent star and with this code we simply recognize simple comments we just break him up into multiple regular expressions so if we see open paren star in y y initial then using a y by begin comment we go into the comment state now that we're in common state only the the regular expressions that are prefixed with uh comment and triangular brackets only those regular expressions apply so when we're in common state and we see cl star closing parent we go back into a state y by initial so we we saw the end of the comment and if in common state we see any other character then we simply ignore that character because we want to ignore comments altogether so this would be one way of how comments could be broken up the um now in those code fragments it would also obviously be possible to add other statements so in that second section of the input file you could have a variable for the nesting depth of comments and then [Music] when when you see the beginning of a comment starting in y by initial then you initialize that comment nest encounter to zero if you're in common state you see another beginning of the comment you say simply increment that counter if you see the end of the comment you decrement the counter and if if the counter becomes zero again then you go back to state by initial so that would be how we could handle uh nested comments uh by the way open paren star and star closing parenthesis how what comments would look like in the lang functional language ml in a tiger i believe it's uh slash star to star slash and in jlex all regular expressions that are not prefixed by a state in triangular brackets they all apply in all states this is unlike other lexical analyzer generators for example lex the c lexical analyzer generator has two different types of states declarations one for uh inclusive one for exclusive states so you could you could have you could declare a state a comment state in lex such that it only applies to to the common state and that the default for all for everything else is that comment does not apply so that means you wouldn't need to explicitly list y by initial everywhere but uh jlx does not have that feature so in jlx it is necessary to put wi-fi initially in front of everything other than what goes inside comments in the inside strings this is the idea behind uh start states now uh conceptually this is the syntax for it conceptually how can we view start states we could view start states as being a high level automaton we'll see um in one of the next videos that they're coming up uh soon how automata are constructed for individual regular expressions but we can think of of start states as we have uh that state y by initial which is the default state that the lexical analyzer starts in and when we see the beginning of the comment uh open paren star we go into the common state if you see any if you see a star closing paren we go from the common state back into y by initial if you see anything other than a star closing paren and common state we stay in the common state and if we see the keyword if in state y by initial we stay in state y by initial so this is how how you could visualize the behavior of how we are transitioning from one state to another and the operator for transitioning is that function call while i begin to to switch to a new state


Escape Character Sequences: https://youtu.be/f3W--O9ivQQ
so how do we handle escape character sequences in strings uh just about all languages have escaped characters where inside a string you could for example have backslash n and that backslash n uh corresponds to the new line character the lexical analyzer uh reads one character at a time from the input file so as the lexical analyzer reads uh string constants it makes sense to uh to immediately translate all those escape character sequences into the corresponding character that they represent so for example if you have a two character lexeme which might be uh specified using the regular expression backslash backslash n so backslash n in jlex also means the new line character so if you want to have if you want to have the if you want to have a regular expression that corresponds to the backslash character followed by the character and we could either write the backslash n in double quotes which does not mean the new line character which would be a backslash followed by n or we um we could escape the backslash uh by putting another backslash in front so backslash backslash represents the single backslash character and then the n afterwards that's the character n so we have two characters the backslash character and the character n that would be read from the input file and that then should be translated into the new line also known as the line feed character in ascii which is the the character with character codes 10 10 decimal so how do we do this um recognizing the entire string with a single irregular expression would be possible but then uh we didn't do any of that translation so then we would need to process the entire string again and then translate the characters which would be a lot of additional memory accesses which are expensive so what we do is we again use start states we would have a string state where when when we see the initial double quote for the string then we go into string state and we initialize some form of string buff in which we accumulate the characters inside the string any escape character sequence is translated appropriately and the correspond the character denoted by the escape character sequence is added to our buffer any other character anything other than an escape character is added to that buffer and then when we see the final double quote we exit the string start state uh package up that whatever be accumulated in the buffer inside the token and return the token to the parser now uh for understanding how this translation works we need to understand what the ascii character code looks like and how we can create individual characters from that table and of course that which escape character corresponds to which ascii character that again uh differs from uh one language to another language so we also need to carefully cross reference the language specification about what escape character sequences we have in our language for the ascii character table there is a convenient wikipedia page which i have right here but there's all kinds of other useful ascii tables if you google just ascii or your google ascii table there's for example ascii table dot com or ascii hyphen code.com or you could use man ascii in unix the command man is uh gives you the manual page so on on the classes server with this command man ascii on in the shell you'll get the the manual page for the ascii table and these main commands also work as google searches and you get all kinds of additional uh ascii tables but let's have a look at the wikipedia page scroll down here is where the actual table starts so we have the character code in binary octal decimal or hexadecimal ascii is only specified for seven bits not eight bits only seven bits uh extended at the extended ascii table that then would be would be eight bits where uh ascii code with uh the character with ascii code zero is the null character and then if we go down for example if for the light blue column decimal 10 that's the line fit character decimal 13 that's the carriage return character at the end of a line in linux or unix you have a single line feed character in ancient macos you had a single carriage return character but that's essentially extinct modern mac os is a unix variant so you have a single line fit at the end and in windows the end of a line is denoted by the two character sequence carriage return followed by line feed on ancient typewriters uh carriage return was when the carriage that that mounted the roll of paper would be moved to the beginning of the line again and line feed is when the carriage rotates to its vanes to the next line so those uh those ascii characters were designed for for operating a a tailor type writer for communicating with a uh with a computer in ancient times and so we still have the leftovers uh reflecting that many of those uh special characters are not needed anymore not used anymore but some of them are still uh in active use for example with essie code three we have uh ctrl c which is end of text which happens when you type when you hold down the control character and type a c or ctrl d is in the file or end of transmission for the tailor type writer that's that closes an input file uh in in linux for example control g is the bell if you're logged into a linux terminal or as or macos or seguin terminal on windows and hold down the ctrl key and type ctrl g the terminal will ring will dig at you backspace is ctrl h ctrl i is tab or horizontal tab etc then we have vertical tab form feed a form feed goes to the next page on a on one of those uh ancient uh printers where you had the perforated strips uh on the side and the paper would uh would fold so it's endless paper that folds and with a form feed you'd go to the next page of most languages or many languages recognize form feed as a special white space character for example in c new code like the source code of gcc uses form feeds to go to the next section of code in a large file and there are editing commands so you can conveniently jump from one section to the next section for for browsing your uh for finding your way around large files if you don't have a modern fancy editor then there's a whole bunch of characters that are not actually used very much and then all those control characters go all the way to ascii code 31 or hexadecimal one f and then finally the delete character is the character with ascii code 127 and then uh in between between 32 and 126 that's where you have the printable characters space is 32 then there are a bunch of special characters then the digits start go from 48 on or hexadecimal 30 on to exodus about 39 again special characters and at decimal uh 64 or hexadecimal 40 that's where you first have at followed by um by the uppercase characters that a few special characters and then with um uh with uh from 60 hexadecimal on that's where you have the lowercase character those individual columns that's a little bit confusing here if you look at the top of the page um different um in different years of the specification different symbols were used so for our purposes it would be the right most uh one that that's the one that counts that's in common use now the so what does this mean for the tiger compiler for this we need to look in at the language specification on the assignments page on moodle down here is the language specification and on page six at the very top that's where we have the the control characters defined so in tiger backslash n just like in c or java uh denotes the new line character and again just like in c or java backslash t is the tab character so those would be handled the exact same as uh in c or java and for creating the new line or the tab characters you would use the corresponding uh uh string constants or character constants backslash n and backslash t in in your java code fragments in order uh to con to create those characters then it tiger says that we have we have control characters with the syntax backslash carrot fault by c and then it says for any appropriate c so this is for all control characters and according to the wikipedia page control characters control code chart would be all the characters with ascii code 0 to 31 plus ascii code 127. the reference implementation for our compiler only recognize control characters 0 through 31 but you feel free to implement 127 as well but it's not strictly necessary and how are those control characters denoted backslash at is 0 backslash a all the way to backslash z and then afterwards we have left square bracket backslash right square bracket carrot and underscore let's look at the uppercase characters for the printable characters after z we have uh left bracket backslash right bracket carried another score so the exact same characters so for recognizing those uh control characters if you're if after the carrot you have a carrot a character with uh ascii code um 64. all the way to um 95 then uh you sim you simply take that ascii code and subtract 64 from that and you end up with a character for the control characters that is you'd have exactly those uh those character combinations carrot add carrot a and so on and uh the same thing for lowercase characters so for lowercase uh letters uh a through z you should treat them the same as capital a through uh capital z the that's this uh here then with backslash ddd we could specify an ascii code with three decimal digits now with three decimal digits you obviously can have characters uh larger than uh 255 so there should be an error uh an error check that you can only only have uh three decimal digits that uh spell out a number uh between zero and um uh in 255. the and then you would simply translate the uh cast that number from an int to a character in in java just like in uh c or other languages in the c family you can do arithmetic with characters so you can compute with characters as being small uh just like small integers in java characters or unicode instead of ascii uh for you ascii characters or 8-bit or extended ascii or 8-bit unicode or 16-bit characters and unicode allows you to encode a whole bunch of different languages for the english character set the higher eight bits in unicode all zero so so that means the unicode characters you simply ignore the the top eight bits they all zeroes and you take the lower eight bits and uh treat them like uh ascii codes then backslash double quote that's a double quote character so you can have double quotes inside strings backslash backslash is the backslash character itself and now this here is weird backslash f to f followed by another backslash and then we need to read the text here it means that all those f's of format character formatting characters that is space tab new line form feed i would add carriage return to this uh all the typical white space characters so if you have a backslash followed by white space characters followed by another backslash then all of that should be ignored this is the tiger mechanism so you can have large strings formatted so that the string spans multiple lines and then at the end of one line you simply have a backslash and that in the next line you have a bunch of spaces then another backslash and then your string continues so that means any white space between those backslashes needs to be ignored and for this you need a third start state so for a total of three start states that we um that we need for our uh for processing tiger and this is it uh for uh for recognizing uh escape character sequences and um with this you should have everything you need for uh for writing the lexical analyzer the remaining videos for this section are more a theory what what are the different algorithms that jlex employs for for translating uh regular expressions to a non-deterministic deterministic finite automata and eventually spit out something that uh actually can act as a finite automaton


DFA: https://youtu.be/i9M9XUoFzmA
in an earlier video we discussed the overview of how chilex works that it takes regular expressions and eventually somehow uh translates them into a deterministic finite automaton or dfa which then can act as um as the lexical analyzer engine the the actual scanner that reads the input so what's a dfa anyway the easiest way to visualize the dfa is using these drawings where circles or states transitions between states or or edges and the one state in each automaton has an incoming edge out of nowhere that state is identified by the incoming edge out of nowhere as the start state and for all other transitions we only take an edge from one state to another when reading an input character and the states with a double circle around are final states those are states where we recognize a um we recognize a electime and then generate the appropriate token and uh hand it back uh to the parser so the automaton in the top left that would be an autumn for recognizing the key word if the automaton on the top right would be enough an automaton for recognizing identifiers in this example to fit it on the slide it's uh those are simplified identifiers so we only have um we only have letters lowercase letters and digits now um the starts they the final state has an edge to itself this means that after the first letter we already arrive in the final state but then if there are more letters or digits we keep reading we keep reading these additional letters or digits until we find something that's not a letter digit anymore that is we keep reading additional characters and stay in the final state and only once we see something that does not fit anymore then we return that token this is um this is what we've seen uh previously about the [Music] the strategy of um of jlex we read as many characters as we can using a single regular expression and only then do we return the appropriate token now if you have uh two automata one for the keyword f14 for identifiers those automata could be combined uh into the combined automaton at the bottom that recognizes uh both the keyword if and identifiers so we start in state one if we see the letter i we go into state two state two is the final state because here we could recognize uh the identifier i if after the ivc and f we go into state three that's the final state where we would recognize uh the keyword if if in state one we see anything other than an i we go to state four again it would be final state i should say anything other than i but still being a letter or digit for the purpose of this example and similarly from stage 2 if you see anything other than an f we go to state 4 and from state 3 if you see anything if you see additional letters or digits we go to state 4 and in stage 4 we keep we keep reading additional letters or digits until we hit for example end of file now uh how could uh session automaton be implemented in easy implementation is as a two-dimensional table where we have the state as one dimension and um the the input character as the different input characters as the the second dimension where the table tells us what's the follow state so if you're in states this is um just as as an example it's not intended to mirror the the previous automaton necessarily if we are in state 0 we might stay in state 0 if you're in state 1 nbc and a we go to state two in state one at b we go to state two uh and so on now uh for a lexical analyzer engine we also would need an additional table or an if and else chain that says if we end up in the final state and we can't go further what's the corresponding token that we would need to return now um when writing alexa when writing it find a state automaton or deterministic finance automaton by hand uh it building such a complex uh table or potentially fairly big table that's uh not gonna be uh easy as a human programmer it happens to be that this is what jlex does but if you want to write an automaton by hand an easier solution is using a while loop and the switch statement this is the the implementation that is normally recommended in compiler textbooks so you have a state variable you might initialize that to zero and um and then with a loop as long as you haven't seen in the file then inside the loop you have a switch based on which state it is you execute the corresponding branch then break out of the switch statement go to the next iteration of the loop and continue and if you have a state transition from one uh from one state to another you simply have an assignment where you assign the follow-up state to that variable state now this is the recommended code in many uh books is it easy to read well yeah it's nice and structured but it's not necessarily easy easy code to understand unless you know that this is a finite state automaton and with deterministic final automaton though those terms are interchangeable and and you happen to also know you happen to also have the documentation that says what the state looks like another possible implementation instead of the loop and the switch statement you could simply write spaghetti code with a bunch of gold tools where each state corresponds to a label that with the code tools you go to other states and i would argue that this is the only case for for having a real proper spaghetti code with go to where you jump all over the place that's uh that makes sense and it's not any worse uh for for understanding the code if it is properly documented if you have the documentation that says this is a deterministic finite automaton and here's what the automaton does without the documentation of course the the code is completely unintelligible but with the documentation uh it's not uh necessarily any worse than uh the while loop and the switch statement now another implementation that's out there the in the design patterns literature there is a state design pattern and the state design pad in the state design pattern you have a similar structure as uh as here where you have a variable that keeps track of which state you're in but the state itself is an object so when when you assign a new state to the state variable you assign a new object to the state variable and the object has the appropriate methods for for what actions need to be performed in the state and what the follow-up state would be for writing a lexical analyzer i think the um the implementation with uh objects as state is overkill but there are uh other scenarios where a finite state automata are used for example many embedded systems uh underneath the hood can be implemented as uh as deterministic finite automata and there it may make sense to have an implementation with a class hierarchy for all the different states and and you select the appropriate state by assigning the current state object to a state variable that's more useful if in in each state you might have a few different operations that could be performed such as reading different input signals for uh for an embedded system etc for lexical analyzer where all you're doing is read one character and go to a follow-up state that's too complicated now these are the possible implementations of um of a dfa now how do we get from uh regular expressions all the way to such an implementation and in order to do this we first need to have a look at non-deterministic finite automata because uh those are easier as an intermediate representation to go from uh the regular expression to nfa to non-deterministic final automaton and then from nf8 to dfa and we'll have a look at those steps next



NFA: https://youtu.be/15-S8r6Rw9Q
No transcripts



RE to NFA Algorthim: https://youtu.be/b8KUcsl9zRY
for the purpose of lexical analysis the reason we are interested in interested in anaphase is that regular expressions can be translated very easily into interface and there are two uh two algorithms that can do that the older one is thompson's construction that's uh presented in the dragon book uh the book compiles by aoseti aldman and lamb and um it has been called thompson's construction for a very long time although i heard that uh actually it seems to predate uh thompson's presentation of that algorithm but at any rate it's called thompson's construction in the dragon boat this is the algorithm that i prefer because it's uh it's a lot easier to do manually it's easier to keep track of there's a different algorithm in in our textbook i'll have a look at this uh afterwards so let's first have a look at thompson's construction we construct the automaton uh corresponding to the syntax of the regular expression so if our regular expression is uh is epsilon or the empty regular expression then we construct uh an automaton in nfa with a start state a final state and an epsilon edge in between them if a regular expression is a single symbol say the symbol a then we construct an nfa with a start state a final state and an edge with uh labeled with the symbol a now for all the cases in this algorithm we always are going to construct an nfa with a single start state a single final state well there can only be one start state in an automaton but a single final state and with appropriate edges now if the regular expression is the concatenation of two smaller regular expressions so we have this regular sub expression n followed by the regular expression m then we take the automata for n and m those are indicated by those uh ovals so we have the automaton for n with the start state and final state the automaton for n with the start state in the final state and the final state of the automaton for n becomes the start state of the automaton for m so those states will overlap and for the combined automaton the final state of m is the final state of the combined automata fairly straightforward the next couple cases are a little bit more complicated so if the regular expression is constructed using alternation we have a regular expression n or m so we have two sub uh expressions uh n m and uh we have the uh the vertical bar as a choice operator in between them in this case we create two new states a new start state and a new final state and four uh epsilon edges we have two epsilon edges from the new start state to the start states of the automata for n and m and we have two epsilon edges from the final states of the automata for n m to our new final state the the case for a repetition so if you have a regular expression n star is similar we add uh two new states and and four epsilon edges so we add a new start state a new final state an edge from the new start state to the start state of n and edge from the final state of n to the new final state then a backward edge from the final state of n to the start state of n in the forward edge from the new start state to the new final state the the way thompson's construction is is designed is that the sub automata for n and m could also have been constructed manually so if um suppose uh this in in the case of alternation the automaton for n has been constructed manually where there is a back edge a loop back to the start state of n if we would not introduce a new start state but if instead we would combine the two start states of in an m then with a loop back to the start state of n we then could go into the uh automaton branch for m so it is not safe to if the stop automata have been constructed manually it would not be safe to uh to combine them without these additional states and additional edges if they the whole automaton would be constructed with the algorithm then um some of those excellent edges may be uh maybe unnecessary some of the states may be unnecessary but i still think the easiest way to think about this algorithm is just blindly follow it whenever there's alternation or reputation two new states four new epstein edges let me repeat this two new states four new epsilon edges just uh remember that and for repetition the backward edge is on the inside the forward edge is uh on the outside so we don't have an infinite loop now um what's the difference between thomson's construction and andrew repels algorithm from our textbook [Music] that uh that algorithm is uh the the drawing for the different cases of the algorithm or too complicated to draw in a powerpoint you can have a look at it in the textbook it's figure 2.6 on page 26 of the textbook and in appel's algorithm he does not have for the sub automatic fragments that are being constructed he does not have his start state instead he has an incoming edge out of nowhere with a label that is the incoming edge either has an epsilon or another symbol as a label but there's no proper start state and these automata fragments without start state are then combined into a larger automaton and at the very end a uh a new start state a start state for the whole automaton is added so um for comparison thompson's construction has for each sub-expression of the the regular expression always has a start state and a final state for the corresponding nfa appel has a partially constructed automata for each expression with the missing start state and then adds the the final as the start state at the very end to the final automaton advantages disadvantages thompson's construction is easier to follow uh and works even if the sub automata had been constructed by hand the pals algorithm results in fewer states and fewer epsilon transitions but it's not really a big deal it's always possible to optimize the automaton afterwards and uh reduce the number of uh transitions so i um it seems it would be implementation-wise it's like a more complicated data structure to represent the pel's algorithm uh for a maybe a slight uh improvement in speed and at the end after the automaton is optimized and unnecessary uh edges or states have been uh optimized away then it comes out to be the same thing



RE to NFA Algorthim: https://youtu.be/MBsF67T5YMo
so let's have a look at an example for how we can translate the regular expression into an nfa using thomson's construction suppose we have the regular expression a b star abv again this is taken out of the dragon book but i show all the steps here for constructing the automata for constructing that nfa the easiest is a bit because the the rules are defined recursively the easiest is to go to the base cases of the recursion that is to the uh to the smallest sub expressions uh wherever you have the the deepest nesting structure of the regular expression so in our case we have a b star so we have uh a repetition on the outside with alternation in the inside and then a and b as separate sub-expressions on the varying side and and we start with those so for those a's and b's uh for that a and the b inside the parentheses we have two automata one with an edge labeled with a the other one with an edge labeled with b now we work our way out outwards the next step is alternation a or b we add uh two states for epsilon transition it will start state with epsilon transitions to the start states of the sub automata for a and b and a final state with epstein translate transitions from the final states of the sub-automata for a and b to our new final state next we add the repetition again we add two new states for new epsilon transition a new start state on the left with an absolute transition that goes to the automaton for a or b if no final state on the right with an epsilon transition from the final state of aop to the to the new final state the backward edge on the inside the forward edge labeled with epsilon on the outside next we add our tail that abb after the start and we simply can uh concatenate them one after another there is no uh need or no reason for from the algorithm for adding any additional epsilon edges here so we simply concatenate the other pieces now all we have left to do is identify what's the start state what's the final state in this case um the the picture shows it because the algorithm always con has one start statement final state but well we want to identify it so we label the start state with the incoming edge out of nowhere we label the final state with a double circle around and then we number uh the states because we'll be uh referring those to those state numbers uh later on when we translate this nfa into a dfa so again to to repeat this start inside out and follow the algorithm uh always have a start state and the final state for each sub expression and for or and star two new states for new epsilon transitions


NFA to DFA Example: https://youtu.be/lbAY6uqq9fs
now let's translate uh an nfa into a dfa this is also called the subset construction algorithm previously when we talked about how an nfa could be implemented directly we said that an implementation would need to keep track of all the possible nfa states that we could be in at any given point in time and so this is the same idea for the translation algorithm that a set of nfa states corresponds to a single dfa state so for example if we have a state one with epsilon edges to two and three so we have that set of uh nfa states where from one we can reach two or three without consuming any input in the dfa when we make a state transition we always need to consume input so therefore the set of states 1 2 and 3 would have to would have to correspond to a single state in the dfa now for for understanding how this algorithm works let's just uh go through this uh in an example and then afterwards we'll have a look at the algorithm so uh we again use our example for the regular expression a or b star apb that's the same nfa that we had constructed previously now uh for translating this into a dfa what we do is we keep track of uh two things we keep track of the dfa states will label them a b c d e and so on and uh with which a dfa state corresponds to which set of nfa states and in addition we build a transition table where in the transition table we have rows for the states and columns for the input symbols so we start out with the start state of the start state of the nfa which is state 1 and from state 1 for constructing the start state of the dfa we need to take the start state of the nfa and uh add all the states that can be reached on epsilon without consuming any input because the uh the start state of the dfa has when we start out in the start set of the dfa we haven't consumed any input yet so from state one we can reach states two three five and with the forward edge for the repetition state eight now the way i'm writing these sets uh state one is underlined because state one is reached directly and the the other states two three five and eight are reached from state one using epsilon edges now um at the same time we construct our table we added a row for state a we have our columns for inputs a little a little b and now what we do is we try to fill in that table so as a first step let's uh let's see what happens when we have in our table when we have a transition from state a on input a little a to do that we need to look at all these sets all the states in set a where do we have uh transitions from um from any of the states in a on input a little a so here we have um we have a transition from state 3 to state 4 on input little a and we because 8 is in the set as well we have a transition from state 8 to state 9 on input level a so states 4 and 9 can be reached directly those are again underlined and uh from those states from states four and nine then we look at what other states can be reached with epsilon from state nine there are no outgoing epsilon edges from state four we have epsilon in epsilon edge to state seven from seven we can go to the right to state eight or using the backward edge we can go to states two three and five so uh the all the states that we can add with epstein edges or two three five seven and eight next let's try to fill in uh the the second slot for uh for row a so where can we go on in from state a on input little b uh again for all the states in set a one two three five and eight what are all the outgoing edges uh on input little b we only have this one uh this one edge here where from state five we can go to state 6. and so again 6 is underlined that state can be reached directly afterwards we add whatever we can reach on epsilon so from 6 we can go to 7 and then to the right to 8 over with the back edge to 2 3 and 5. so c is the set 6 followed by 2 3 5 7 and 8. now um let's uh let's move on for our row b so where can we go from state b on input little a so again uh we need to look at all these sets in state b where we do have do we have transitions in little a three is in the set so therefore we can go to four using this transition at the top and in state b we also have eight so from eight we can go to state nine so we can reach four and nine directly four and nine that sounds familiar we've seen this before those are the underlying states that we can reach directly so we don't need to construct the full epsilon uh the full set of states that we can reach in epsilon four and nine are reached directly we look at which states have four and nine underlined that's b so therefore from b an input little a we loop back to state b now from b on input little b that gets more complicated we now have the transition from 5 to 6 because state 5 is is in set b but we also have the transition from 9 to 10 which we didn't have previously because state b has uh contains state 9 and so therefore we have a transition to 10. now the set of states 6 and 10 that's something new we haven't seen this before so we'll give it a new name let's call it d uh we write we write down our set d with a 6 and 10 underline followed by whatever we can reach in epsilon from 6 and 10 from 10 we can go anywhere in epsilon and from 6 again we can go to 2 3 5 7 and 8. and we put d into the table now if we continue from uh state c uh the the outgoing transitions are exactly the same as from state a from a state d on input a we again go back to state b and from state d and then put b now that's different again we have in d we have the state we have state 5 so we have this transition from 5 to 6 but the state d also contains set d also contains state 10 so from 10 we can go to 11 so the set 6 and 11 that can be reached directly that's again something new let's call it e and um and then again we add whatever we can reach an epsilon which is two three five seven and eight and now we also need to fill in the row for e and again the outgoing transitions from e are the same uh on a to b and on b to state c now to wrap it up we need to figure out what's the final state of our dfa so the final state of the dfa first of all the dfa can have multiple final states uh any uh any set or any dfa state that contains the final state of the nfa is going to be a final state in the dfa in our example only e contains state 11 so therefore only e is a final state and i identified this in the table by having a larger a thicker horizontal line before e to to indicate the grouping that a b c d and non-final states is a final state and then the same information that we have in the table is then simply translated into into a pictorial representation of the dfa with the state's abcd e and all the transitions copied from the table

NFA to DFA Algorthim: https://youtu.be/OsPR8H1slsE
so now that we've seen how the nfa to dfa translation works on an example let's uh let's try to formalize that algorithm we have two helper functions epsilon closure and dfa edge the epsilon closure takes its input a set of states those would be nfa states and as output it uh it returns all the states that can be reached uh with epsilon edges from state s so the states instead s itself as well as whatever can be reached with epsilon edges so that's the epsilon closure now at dfa edge for constructing a dfa edge as input we have a set of states s and an input symbol c and the output is a set of states t such that we have an edge in the dfa from uh set s to set t on input little c so we're constructing that dfa edge the algorithm for this is for each state in s we follow all the transitions uh on a symbol c or the outgoing translations in symbol c and then afterwards calculate the epsilon closure for accumulating the states for state t now the actual algorithm the start state of the dfa is the epsilon closure of the start state of the nfa and then in a loop we pick any dfa state s and an input symbol c construct a dfa edge from state s on input c let's call that a new let's call it state t if state t did not exist yet we added to the dfa and then whether it existed or not we add an edge labeled c from state s to state t to our dfa and we continue until normal edges can be added and that's it by the way the uh the exit for this algorithm is until no more edges can be added not until no more states can be added added or until the transition table is full uh it is part in the previous example the transition table ended up being full because there were so many epsilon edges all over the place but in general it's not necessary for the transition table to be full there can be blank entries in the transition table so this is the algorithm for what we had gone through in an example earlier


DFA Optimization Algorithm: https://youtu.be/_lkaRx2SOUo
No transcript


DFA Optimatization Example: https://youtu.be/feCxlwzo-3g
No transcripts


Language Classification: https://youtu.be/FU7KC9kS854
when talking about compiler theory normally uh the anti-material would start out with a discussion on what's a language formally defining a language looking at different language classifications etc for this course we didn't have time for that because we wanted to get the start or we needed to get started with the first programming assignment as soon as possible so now that we covered the theory for lexical analysis let's go back and uh talk about what's the language anyway so um think about how you might you might explain what's a language to non-computer scientists and non-mathematicians so for example say your grandparents in a way that's easy to understand and so that it covers both programming languages and natural languages now here might be a good time to pause the video for uh for a minute or two to think about how it explained this on the next slide we'll have the solution so how would we define a language we start out with an alphabet for english it's um the 26 characters a to z plus some punctuation characters for programming languages we start out with a character encoding such as ascii or unicode and then a language is simply the set of all valid strings over the alphabet now this definition is not particularly helpful because it doesn't tell us how we recognize whether a string is valid or not but so with this definition still works so the the language english is the set of all um of all english sentences uh or maybe if you wanna include um things like uh table of contents or or indices is part of the language as well then you could say the set of all possible books while the language java for example would be the set of all valid java programs and we can define this language for any mechanism that describes a structured set of characters over an alphabet so for example uh for our regular expression a or b star a b b oops there's one parenthesis to many uh that ex that regular expression describes a language namely the set a b b a a b b a b b and so on it's going to be an infinite set because we have that uh that star in uh in the regular expression but it's still a very structured set it's described over the alphabet that only contains the characters a and b or the next regular expression 0 or a digit in the range of 1 to 9 followed by zero more digits in the range zero to nine those would define uh the natural numbers now for languages like java or english the description mechanism would seem a little bit more complicated so for classifying languages there's a well-known uh language classification based on the complexity of the language by noam chomsky uh from uh emeritus professor from mit let's briefly have a look who norm chomsky is it's uh this fella here you might have uh seen uh talks of him i've randomly seen like three talks to him on on tv years ago there's a lot of lots of his talks on um on youtube for example he's writing books faster than many of us can read his uh published uh over 150 books by now the mid when the count was around 100 it was something like a third on linguistics two-thirds of political science the last 50 books i don't know what the topic they on but he was a very a prolific researcher in linguistics and later became more active in political science and uh propagating his political views which uh even for liberal people might be too liberal but it's if you get to see uh some of his talks he's very interesting to listen to because he's arguing his uh his views uh very well for example he's very outspoken against what uh what israel is doing even though he is jewish and lots of interesting views although um chances are you'll probably disagree with him uh on at least some aspects uh politically but his uh his professorship was on um it wasn't linguistics and that language classification came from linguistics if you look up language classification on wikipedia you'll find the same four categories plus several other more fine-grained categories in between but for programming languages these are the most important ones regular languages context-free languages context-sensitive languages and unrestricted languages and each of those languages comes with tools for recognizing for recognizing words or sentences in that language and has different uses in compilers so a regular language can be described using a regular expression and can be recognized using a finite automaton and for compilers we use regular languages for lexical analysis because finite automatons are very simple and uh and because for most languages the set of all like themes in the language forms irregular language so it's it's the appropriate tool context-free language uh can be described using a bnf grammar and uh the the tool for recognizing that would be a a parser and or a either recursive descent or a push down uh automaton parser and the use of it is for parsing purposes in the compiler now everything that a regular expression could do that is finite automaton could do a parser could do as well but we split the two uh because uh it makes the parser a lot easier if uh lexical analysis is split out with a different tool and then a context-sensitive analysis there would be a tool called the rewrite rule systems but for practical purposes that's that's too complicated and too expensive to work with but most languages are context-sensitive languages especially statically typed languages type checking is a context-sensitive property so we use context-sensitive languages for semantic analysis and but normally have handwritten uh programs for that context-sensitive analysis and then finally what goes beyond context-sensitive would be unrestricted languages with the tool being an arbitrary touring machine or an arbitrary program now where do java or english fall in java would be a context-sensitive language english i always thought that it was a an unrestricted language because natural languages are more complicated than um than programming languages but then i saw a reference somewhere uh that said that natural languages would still be uh context-sensitive so um i'm not a linguist i don't know uh the fine distinction of what might make it unrestricted versus uh context-sensitive but those are those are the main categories for classifying languages so now what are uh some of the differences between them especially for the purpose of compiling so the first thing between regular and context free languages the thing to remember regular languages can't count so the language a to the end b to the end that is the string of all uh of all the set of all strings containing equal numbers of a's and b's first all the a's then all the b's you can think of this uh as uh abstracting matching parentheses or nested comments that's not regular why if you imagine a finite automaton recognizing that language a star b star that's different that would allow arbitrary arbitrary uh orders of a's and b's enforcing this order while still requiring that you have the same number of uh a's and b's the only way you could do this is that you have a different state for each n so from the start state with one a you go to the next state with b you go back to the start state from the next state with one a you go further to the right with b you come back but uh if n is potentially uh unlimited then it would not be a finite state automaton anymore and so therefore it's not irregular language context free languages can't remember counts so with a parser something like a to the and b to the end like matching parentheses uh matching beginning and end of comments that would be easy but a to the and b to the and c to the end that does not work so the parser can count up and back down to zero but it can't remember how many uh for example a's we have for them comparing against the number of c's and where do we have that in programming languages an example would be um for type checking function calls suppose we have a function a declaration that says we have two parameters and then for all the following function calls we need to check that we have the same arguments as we had the parameters where the number of parameters might be a the number of arguments in the first function call that would be b the arguments in the second call that would be c it's uh not exactly what uh what the abstract notation looks like but this is an example that's uh sort of similar that demonstrates that type checking cannot be done with a context-free parser



Scanning Problem and Summary: https://youtu.be/TVKERkOC6ic
so for summary on lexical analysis let's uh look at a few lexical analysis problems so first of all in tiger one of the problems was that we had nested comments and nested comments is not a irregular language mechanism so it cannot be recognized or cannot be described with a single regular expression but we have a solution for that in j-lex just like in probably all lexical analyzers namely uh that of start states so by using start states we're able to break up that the nested comments into smaller fragments and each of the smaller fragments can then be described with a single regular expression next was strings with escape character sequences technically that would be irregular it would be possible to write a single regular expression that that recognizes all possible strings and recognize and describes all possible escape character sequences that might happen inside strings so for example that the string with um with backslash one two three would be accepted but a string with backslash uh 500 would not be accepted as legal so that would be possible but that regular expression would be quite unwieldy and uh and difficult to write furthermore describing strings with a single regular expression doesn't help us with translating the escape character sequences into the single character that they represent which we need for further processing now it would be possible again in theory to use a single regular expression put the entire string in the buffer and then uh process the buffer again and translate all the escape character sequences but that's um that misses the idea of lexical analysis we're already reading one character at a time uh it would be a too expensive to put in a buffer and then process the same buffer again because memory accesses are uh are expensive so again the solution is we use start states to help with uh writing that translation how about problems in other languages in the language pl1 uh we had this as the first interlanguage when i was in college it is a horrible language uh pl1 did not distinguish between identifiers and keywords so this uh sentence here if if equals then then then equals else that is actually legal pl1 code uh now the first if the middle then those would correspond to keyboards all the rest would be variables and yes it also uses the same operator for comparison and for assignment uh what does this mean for lexical analysis well let's just let the parser deal with it so that means lexical analysis would only recognize uh identifiers it would not recognize keywords and simply hands identifies onto the parser but then the parser has a harder time parsing because uh in order to determine which language construct it is it can't just look at the keyword uh it needs to unpack the identifiers so that's uh there is no enumeration constant in the token that tells the the parser which branch to go whether this is an if statement or whether this is the start of an assignment uh we actually need to look inside that that identified to see what it is uh fortran fortran has an interesting way uh to write uh counting loops like the equivalent of for loops in in c uh so the idea is you have a construct 220i with 20 as a label and then the start value and the end value for for the loop then you have the loop body and then at the very end for the last statement of the loop you have that label like 20 and then either the last statement of the loop or just a no up statement called continue that means we continue the loop iteration now the problem is for this to be a a fortran loop it actually should be a comma not a period after the one this is a fairly famous era that uh people had to deal with at nasa uh when i was in college the story was told that because of the sumerian approach got lost but the reason for the marina probe loss was actually an incorrect translation from uh handwritten mathematical notation into um into programming language variable names uh for the um when i was in grad school they tracked down somebody who actually was at nasa in the 1960s when they had this problem so this was in code that was used for calculating the trajectory of satellites around around the globe and then they wanted to use the same code for calculating the uh the flight to the moon and didn't get full precision instead of something like six or seven uh uh decimal values behind six or seven positions behind the decimal point they only got like two or so and um it reportedly took them on the order of two weeks to track down this error because that's that was at the time when programming meant you uh you typed each individual line onto a punch card you delivered the tray of punch cards to the compute center and then you got the output back on the line printer and on that line printer you really can't tell the difference between the dot and the period so why was the error so hard to spot why couldn't the compiler simply print an error message well in fortran white space doesn't matter you can have white space in the middle of identifiers let me repeat that you can have space characters in the middle of identifiers because they wanted to be able to do things like write go to as a single word or as two words go space two to make it uh more similar to natural languages so 220i that would be a legal identifier and 1.10 that would be a legal floating point constant and by the way variables also don't have to be a declared variables that start with an initial character in the range i through n by default are integer variables all other variables are floating points so 220i is a floating point number we assign 1.1 to it and that's a perfectly fine assignment and a continuous statement by itself that's also no a cause for alarm that essentially is like a like a no up so there was there's no um there's nothing in the compiler that would complain about this so what's the solution well it's bad language design the the real solution would be to redesign the language but uh there's too much um uh too much code invested in fortran so it's difficult to to ditch the language a better solution is to simply warn programmers of common errors so for example if you have something like uh and identify that looks like it could be the start of a loop but then you have a period instead of a comma you might print a warning maybe in lexical analysis to tell even lexical analysis or in parsing to tell the programmer that hey there might be a problem also with modern fortune dialects um there is a mechanism that it can uh disable the implicit declaration of variables in most uh most professional program fortran uh programmers would do this oh it is in or missing in fortran i'll fix this the finally c plus plus the declaration c x open paren and closing paren that's the fourth declaration of a function x that takes an integer's argument and returns an object of class c and there is no function body because it's a four declaration so in in c or c plus plus when you have mutually recursive functions you need to declare uh one function first without the the function body then you have the second function with the function body and then for the first function you have another declaration now with the body because um the c or c plus plus compiler processes processes declarations linearly unlike what we have in java and so you need those four declarations now uh the next declaration c y open parent five closing parent well if it were a full declaration of a function there should be a type name instead of a an integer constant so it obviously can't be that this is an object declaration we are declaring an object y of type c the object is allocated on the stack so we have the object on the stack for example as a local variable directly instead of having a pointer to the object and with open paren 5 closing paren we call the constructor of class c and pass 5 as argument to the constructor all right so now what about c that last declaration where in parenthesis we have a well it depends is a another variable in that case it would be it would be an object declaration or is a a typed name in that case it would be the forward declaration of a function so what's the solution to this well one solution obviously is for the lexical analyzer to simply not worry about it hand on identifies to the parser the parser still not worry about it and leave it up to type checking to figure out what's happening uh the way this was implemented in gcc when i was working on a gcc for my for my dissertation uh to make the parses simpler the parser was demanding that the lexical analyzer distinguish between three different types of identifiers label names those are easy to uh to figure out they are they're followed by colon and on the at the beginning of a statement type names or other variables so in case of a in case of int that obviously would be a type name in case of c though that would be a tight name x y z would be a regular variable and a well again it depends it depends on how a was declared previously so the way this was implemented is that semantic analysis information that is the symbol table was fed back from simulating analysis back to the to lexical analysis so that the lexical analyzer could look up the declaration of a and based on that decide whether to report it as a type name or as an identifier and if a was not declared yet then the lexical analyzer or a tool in between the lexical analyzer and the parser would look ahead in the token stream using complex analysis including backtracking on the token stream to figure out what characters are coming later in order to guess what would what the identifier right now should be whether it's an identifier or a type name so a very complex language construct to to parse or to to build a compiler for now to summarize uh we split lexical analysis from parsing so that parsing becomes simpler if the parser would have to deal with white space just about everywhere in the uh in the language as well as comments just about everywhere that would make the parser way too complicated uh describing identifiers or integers as uh as grammar rules for the parser that wouldn't be uh terribly complicated but um but the white space and the comment in between that would make the parser very complex and uh find that automata are uh are faster at recognizing uh processing individual characters than the parse engine so that's the reason for splitting off lexical analysis so we describe the set of all the valid like themes as a regular language using a set of regular expressions and then the translation from regular expressions to nfa to dfa to optimize dfa that can be automated and we for our compiler we are using a lexical analyzer generator that automates that translation from regular expressions to a table driven finite automaton and examples of such lexical analyzer generators lex for c flex that's the new version of lex with a little bit of additional features that jlex for java mlx for the language ml and so on there's a there's a ton of lexical analyzers out there or of course you could write the lexical analyzer by hand if you want to have an implementation language for which there is no no lexical analyzer generator available or if you want to make sure that your compiler does not depend on somebody else's tool other than the programming language now jlex you for a strategies for processing input it uses the longest match and rule priority strategies if two regular expressions if one regular expression describes a prefix of another regular expression for example if if the input characters are ifx then the i and f could be recognized as the keyword if and the whole thing effects could be recognized as an identifier whichever regular expression can process the most uh input characters that one wins out so that's the longest match uh strategy and rule priority if you have two regular expressions that uh describe the same input then whichever is listed first in the input file uh wins out so therefore uh identif the the the rules for keywords would need to be listed before the rule for identifiers and uh finally a jlx offers start states for recognizing uh common issues in writing lexical analysis such as um simple non-regular constructs like nested comments or processing strings this is it for for the module on lexical analysis the next module would be on parsing








Parsing Videos
Motivation (5 min): https://youtu.be/bgf5py6LL2I
in this module we are talking about syntax analysis or parsing uh in the textbook that's chapters three and four chapter three is on uh syntax analysis in chapter 4 on building the parse tree so what does a parser need to do essentially just two things recognize the syntactic structure on the token stream typically the parser is called by the main program and then the parser gets one token after another from the lexical analyzer as needed and then as a result the parser produces the parse tree which represents the syntactic structure of the input to motivate what uh what parsing is about and how we want to describe the grammar let's look at a regular expression macros in jlex it was possible to to define macros which then could be used inside regular expressions so for example we could define the macro digits which is the regular expression 0 to 9 plus so that means a number with one or more consisting of one or more digits and then we could define a summation expression as um digits followed by the operator plus repeat that zero more times followed by by more digits so with this simple regular expression we could recognize expressions of the form 10 plus 2 plus 30 for example now let's try to make those expressions uh more complicated say we still have our digit zero to nine plus but now we have uh two additional macros a summation expression is an expression followed by the plus symbol repeated zero more times followed by another expression where an expression itself is either digits or estimation uh in parentheses now just reading at the def reading the definition here this would describe for example the expression 10 plus and then in parentheses uh 2 plus 30. but this is not a regular language anymore because we know that right a regular language cannot express nested uh balanced parentheses so the problem that we have here is we have two uh macros that are mutually recursive and that does not work with uh macros so it does not work with regular expressions so what we need is a functionality similar to that of regular expressions plus recursion and that's basically all there is to context-free grammars except syntactically it's a little bit different so uh instead of uh alternation in the middle of a of a rule or a macro in the regular expression terminology so if you define an expression to be a followed by b followed by either c or d followed by e we have that nested c or d and that does not work in grammar notation so that needs to be factored out as a separate noun terminal so we can have an auxiliary non-terminal aux which is c or d and then an expression is a followed by b followed by aux followed by e and uh that vertical bar for or is that just a shorthand for for two grammar rules with the same left-hand side such as auxes c and auxes d in when we're working with a parser generator we actually do have that vertical bar but uh only at the top level and so literally it is just a shorthand for multiple uh multiple grammar rules with the same left hand side now what about uh iteration or recursion instead of that of cleaning closure or the cleanest doors that star in regular expressions is called clean enclosure named after stephen cleaney a mathematician from last century by the way his phd advisor was alonzo church the the inventor of lambda calculus so instead of that clean enclosure say an expression is a b c star we need to use a recursion where we could say an expression is abc followed by recursively another expression where the expression could also be empty uh here indicated using epsilon when working with a with a parser generator instead of epsilon it would be just an empty right hand side



Context-free Grammar (7 min): https://youtu.be/K9Zg9ZZrojQ
so now we can define what a context-free grammar looks like we have uh terminal symbols uh in lexical analysis terminology they are called tokens or lexemes when talking about grammars the terminal symbols are the symbols themselves in um in the grammar notation so the lexine is the string of characters that's recognized the token is um the object that's communicated between lexical analysis and parser and the terminal symbol is the symbol that represents the token in the grammar notation in practice you can think of this as essentially all being the same thing i will uh often use the terms terminal symbols and tokens uh interchangeably even talking about grammars uh so we have uh terminal symbols followed by non-terminal symbols the non-terminal symbols are the higher level syntactic constructs things like things like expressions statements declarations etc one of the terminal symbols is identified as the start symbol for example an expression for simple grammar in uh for the grammar for language like java the start simple would be something like a compilation unit everything that can be inside a single source file that would be compiled in one piece more interactive languages might have say a single declaration as a compilation unit and just compile one declaration at a time and then link it into the running system like say the language ml and then finally we have grammar rules of the form a non-terminal on the left-hand side of the grammar rule and zero or more terminals or non-terminals on the right-hand side of the grammar rule there are several different variations for syntactic variations for how those context-free grammars are usually written between the left-hand side and the right-hand side you might see an arrow or an equal sign or colon colon equal then the noun terminals might be capitalized or in triangular brackets and terminal symbols might be in inside quotes or all uppercase in our case uh since we have uh the terminal symbols represented as enumeration constants they would simply be all uppercase the earliest grammar notation that was developed that's a buckus naur form or bnf syntax developed after john buckas that's this gentleman here and peter nauer this guy here the two of them have developed buckus now form as a grammar notation for describing the syntax of algol so algo came out around 1960 so they would have uh developed dnf uh sometime late 1950s and pnf syntax is actually standardized i believe there's like an iso standard for bnf and their non-terminal is in the or in triangular brackets between the left hand side and the right hand side there is colon colon equal and then terminal symbols are uh in question marks there is a vertical bar for or and then there's also eb and f or extended bacchus now form where you have square brackets for optional parts and curly braces for repetition zero more times and of course for other language documents you might have a mix of that notation plus maybe a regular expression syntax like star for zero more times a question mark for curly braces or in the scheme reports the scheme language definitions they use in ellipses that means three dots to indicate that whatever came before is repeated zero more times just like the star but with three dots so there are several different syntactic variants an easy way to tell what are terminal symbols what are non-terminal symbols is whatever occurs in the left-hand side of the rule that's a non-terminal and if you have a simple toy grammar then typically the noun terminal in the very first line of that grammar that's also the start symbol so uh that motivating example we had uh in the previous video how could we now write this uh in grammar notation uh in instead of the convenient macro that says digit is uh zero to nine plus we'd need the diff digits plural we need to define uh digit as being zero or one or two so it needs ten rules for defining digits to a digit to be the ten different digits and then digits plural is either a single digit or a single digit followed by more digits or you could turn it uh turn it around and uh say that it says stitches plural followed by another single digit either way works and uh then you we could define a summation expression to be an expression plus uh followed by a symbol plus and then a sum or a sum is uh uh it's just an expression where an expression is uh either digits or a sum in uh in double quo or some in parentheses with the parentheses in uh double quotes little bit about the style of those grammar rules uh there were two different types of recursion and this here this slide is here's just for introducing the terminology right recursion means if the the use of the recurs if the recursive use of a noun terminal is in the right most position of the right-hand side of the grammar rule and left recursion means if the recursive use of the non-terminal is on the leftmost position of the right hand side of the grammar rule we'll have a look at the left the semantics of left recursion and right recursion in more detail later on



Example (5 min): https://youtu.be/eOY71ApuJe0
so let's have a look at a slightly larger example the start symbol in this grammar is s for statement and a statement can be constructed out of a small statement separated by sema colons so we have a sequential execution and then a statement could be an assignment with an identifier on the left hand side the assignment operator and an expression on the right hand side or a print statement with the keyword print and then in parentheses a list of arguments where an expression is either an identifier or a number or a or two expressions separated with an addition operator or a statement for side effect followed by an expression as the value of the expression so that with the syntax open paren as comma expression closing paren that's uh very similar to what you might see and say say with the let expression in in functional languages and a list of expressions is one or more expressions separated by commas here written with using recursion so that now let's have a look at a couple of different uses of recursion here in statement in the first line we have two uses of recursion so um if we have for example more than uh two statements in the input we don't know exactly how to parse it uh should the first two statements uh be grouped together or the second and the third statements be grouped together then for the use of e e occurs on the right hand side of of an assignment or inside that parenthesized expression the la uh the fourth rule for for expressions and uh in either case that's not a problem what is a problem is that again in the third rule for expressions we have expression plus expression so again if the input would be uh would contain multiple addition symbols we don't know whether addition is left associative or a right associative and then finally for the uh for the list of expressions in the last two lines we have recursion on the left-hand side but not on the right-hand side so uh let's look at the different uses we met we talked about this before for the list this is the left recursive uh grammar those two grammar rules for lists it would be possible to write it right using write recursion as well so if for list we write it a list is an expression or an expression follow a comma followed by a list it would define the exact same language as the left recursive version but it could be processed differently in and then finally if you would write oops that should be two if for the ambiguous use of the grammar if you'd have uh l is either a single expression or l is l comma l then it would be an ambiguous grammar and again for an ambiguous grammar we don't know how to parenthesize the input what does this mean for a parser technology ambiguous grammars like that the biggest ver ambiguous version of lists uh i'll fix the slide that i'm gonna upload to moodle or uh here the ambiguous version of statements or the ambiguous uh grammar rule for expressions uh that's always bad news for for the kind of parse technology that we want to use because it would mean that the parser could potentially produce multiple different parse trees and then we don't know which one is the right one so ambiguity is always bad news left recursion does not work if you're building a top-down parser uh write recursion works with any parser technology so there are some constraints on uh what notation we use for the grammar rules depending on the parser technology




Grammar Classes (13 min): https://youtu.be/SVHPLPg8hOA
the advantage of the left recursive grammar rule is that it uh reflects the semantics of left associative operators that is in math where where the expression would uh would be parenthesized implicitly on the left so for example uh 1 plus 2 plus 3 as a left associative operator would be 1 plus 2 in parentheses plus three and uh since many of our arithmetic operators are defined by programming languages as left associative operators left recursion would be would often be the preferable recursive structure but it doesn't work for top-down parsers such as handwritten recursive descent parsers the cost of recognizing uh left recursive grammar by the way is linear uh it is linear in the size of the input file so that's uh the good news for left occurs if uh for uh top-down parses we would just need to um to modify the grammar to make it uh right recursive and then have a more complex tree building code so that the tree would be built as if the grammar would have been left recursive right recursive grammar works with either type of parser either top down the bottom up parser and again the cost is linear in the size of the input file for bottom-up parsers the most common type of parser that we're using is an alley ll1 parser we'll see this shortly and in some cases there may be limitations with that type of parser that we use that we're commonly using but in general anything that a top-down parser can do a bottom-up parser can do as well uh we'll again we'll have a look at this in more detail on the next couple slides for ambiguous grammars when a grammar is ambiguous that means that we could build we could construct multiple different parse trees and for later processing in the compiler that's normally undesirable there are parser technologies out there early sparser named after some guy with last name early the doesn't even have a picture on wikipedia earliest parser is apparently used more in linguistics but not so much in um for compilers just because it's too expensive it's cubic in the size of the input file although it's only cubic if the if the grammar is ambiguous if the grammar is not ambiguous it's only quadratic and if the grammar is uh is nice enough so that it could be uh parsed by regular top down the bottom of parser as well then the earliest parser would be linear as well but still with the higher overhead there's also glr parser general lr parser bottom up parser that um that can handle uh ambiguity the goal of being able to handle ambiguity is that sometimes it makes the grammar easier to develop it makes it makes it easier to write the grammar and then uh as the grammar is being developed eventually get rid of the ambiguity so that we really only end up with a single parse tree what the parses for ambiguous grammars might actually do is produce multiple parse trees where then semantic analysis needs uh to for example type check all of the parse trees and then decide which is the right parse tree based on uh on context information so these the different parser technologies get classified in with symbols like llk lrk and so on uh though those are technically those are grammar uh classes uh but um they're also used as parser types because corresponding to those grammar classes you could have corresponding parser technologies uh so what does this mean uh for ll the first l means we're reading the input left right the second l means we're using left most derivations we'll see this in the next video essentially means it's a top-down parser and the k means we we have k tokens look ahead and similarly lr means reading the input left to right using rightmost derivations with k tokens look ahead and right most is essentially a bottom up parser so this left to right uh reading the input left right what does this tell you that means that this was the orientations who came up with this terminology because for any practitioner it would it wouldn't occur to them that it might make sense to read the input file starting at the end of the file and read towards the beginning but so that left to right simply means we're processing the input file like any sensible computer scientist would do from the beginning to the end of the file now the different parser technologies that are out there ll-0 that's the simplest possible top-down parser which makes parse decisions without looking ahead in the token stream and ll one parser looks at the first token in the input stream to make parse decisions so a typical handwritten recursive descent parser is is an ll1 parser and llk means we can have an arbitrary amount of look ahead and there are parser generators out there such as antler or java cc that can uh that can produce llk parsers the code produced by these parser generators uh looks very similar to the code structure that you'd write by hand for a handwritten recursive descent parser except that that there would be additional code for managing the complexity of looking ahead further in the input stream for bottom up parsers lr0 is again the simplest possible bottom-up parser without looking ahead in the input stream for making parting decisions uh then lr1 is a bottom-up parser with one token look ahead the ll1 or la2 or anything with moto with tokens look ahead results in a really big parts tables though so for practical use even ll1 uh might be a too big for large grammars for keeping the parse table in memory so that's why people developed two different variants slr and ll1 both of which use one token lookahead but with much smaller parse tables so an slr parser is essentially essentially uses ls0 parse tables but then uses a simple trick for uh for bringing in look ahead uh while the lr1 the full lr1 parser has the look ahead encoded inside the parse table and then the lr1 parser where the la stands for look ahead that's essentially an lr1 parser with compressed parse table and just about all the parts of the channel the bottom up parser generators that are out there yak bison java cup emiliac and so on there's a there's a ton of parser generators out there they put nearly all of them generate lella 1 grammars or lala 1 parsers [Music] so if you look at if you look at say the list of all possible parser generators out there there's probably a wikipedia page it would i would guess it would be on the order of around maybe a dozen llk parser parser generators and um several dozens of ll-1 parser generators in the past they used to be ll-1 parser generators uh there may still be some out there but the modern uh top-down parser generators usually can handle arbitrary amount of look ahead now comparing all these different parser technologies there's a nice picture for this i found this image on stack overflow it's very similar to uh to a figure that we have in the textbook on page 68 just um the the corners are a little bit more rounded in the textbook so that uh that was obviously very similar design so um an ll end parser can uh in an ll n plus one parser can parse everything that an lln parser can parse so more look ahead uh lets you parse all the same grammars plus more stuff similarly for lr and lr k parser can parse anything that that an lr minus one parser can do can parse and again for uh that for all possible amounts of look ahead for lr parsers now for the same amount of look ahead uh in one parser can parse everything in ll one parser can parse and lr0 parser can parse everything in llc or parser can parse uh and so on and then slr and ll1 are in between and there are there's some minor trade-offs there are some grammars that would be ll-1 but are not lala one and in particular where this might happen is if you write a grammar right recursively even though write recursion normally works with lr1 uh depending on what comes after that right recursion it may not be lala one while it still might be ll1 uh in um in practice though um any grammar that's uh not uh lala one uh any grammar that's l1 but not ll1 can usually be uh relatively easily translated into an lala one uh grammar uh so that uh in in practice uh lella one is just fine slr and ls0 are not quite powerful enough for for real languages now um in the bigger picture here unambiguous grammars includes everything that that can be parsed with ll or lr parsers but there are more unambiguous grammars that may not be uh uh or anything and then there are there are ambiguous grammars and what uh what we said on the previous slide earliest parser would be cubic for ambiguous grammars uh quadratic for uh in general for on the biggest grammars uh and um linear for anything that can be parsed with an lr parser which is called a deterministic context-free grammar i believe something like that but uh for our purposes what we're interested in is lella one




Parse Trees and Derivations (9 min): https://youtu.be/F8LBthRIIlU
i mentioned parse trees before what exactly are those and what do they look like they're actually two different types of parse trees uh that are interesting when talking about compilers concrete and abstract parse trees let's have a look at concrete parse trees first they're also often called concrete syntax trees or derivation trees as opposed to abstract parse trees or abstract syntax trees and a concrete parse tree represents the syntactic structure of the input but it also summarizes how the input was parsed the leaves of the tree are the terminal symbols if you read the leaves from left to right like in an infix left to right a traversal you get the input back and the interior nodes are the the non-terminal symbols with the root being the start symbol so for example if the input is id call and equal id plus id plus id then for the grammar we discussed uh previously then we can have these two different parse trees because uh the the grammar rule for uh edition was uh ambiguous where at the root we have s as the start symbol it's in an assignment so the children of s are id called equal and e that means the right hand side of the corresponding grammar rule and then for an expression e again because the grammar rule is an expression is expression plus expression we then have the sub trees e plus e but then there's uh the difference uh do we parse the first expression again as e plus e and the second expression is as id or do we parse the first expression is id and the second expression is e plus e the um so each step in the in the tree here the children of a node or the uh correspond to the right hand side of the grammar rule with the parent being uh corresponding to the left hand side of the grammar rule and these steps going from a from the parent to uh to the children that's what's called a derivation step and this is how why parse trees also called derivation trees and uh that's what the derivation is so a derivation starts with the start symbol of the grammar and in each step we pick a non-terminal and replace the not we pick a non-terminal in that string that we develop and we replace that non-terminal by the right-hand side of the grammar rule in this example uh every time a non-terminal is is replaced by something it's underlined so we start with the start symbol of the grammar then we're replacing this with as semicolon s for example and then maybe in the next step we expand the second statement the second s so that's underlined and that's replaced with id column equal e then in the first in the next step maybe we take the first s and expand that into an equal and equal e then maybe we take the first expression expand that expand that to a number and so on now in this example i pretty much randomly picked uh a non-terminal to replace with the right-hand side of the grammar rule from one step to the next what's uh what's a little bit more common is that you'd either consistently always pick the right-most most non-terminal 8 and expand that or consistently always pick the left most noun terminal and expand that with the right-hand side of a grammar rule and those are called left-most and right-most derivations and so what are derivations used for the most important use for language theoreticians is for defining the language that's described by grammar and that definition is actually very simple you take the start symbol all the strings that consist of only terminal stem symbols and can be derived with with one or more derivation steps from the start symbol of the grammar that that set of strings that's the language uh that's recognized or that's described by that grammar and again just like we discussed in the previous module defining language as sets of strings so the language of a grammar is all the strings that can be derived from the start symbol very easily very simply then left most derivations means that in every step we take the left most non-terminal uh in that string of terminals and non-terminals and expand that with the right-hand side of the grammar rule in right most derivations we always take the right most non-terminal and expanded with the right-hand side of a grammar rule and um now if you look at a derivation the derivation is basically a parse upside down so if you take that derivation and flip it on its head where you start with a string that only consists of terminal symbols and then in each step to the inverse of the derivation until you arrive at the start symbol that's what a parse is and um with the leftmost [Music] derivation you're making if you flip that upside down you have the parser decisions in exactly the order in which a top-down parser performs the parts decisions with the right-most derivation if you flip the derivation upside down you have to parse decisions in the same order in which the bottom-up parser would perform those parse decisions so that's where the terminology ll and lr comes from now um what's the concrete so so the concrete parse tree shows uh the summary of those derivation steps what the concrete parse tree does not show is which subtree is expanded first but uh so so a concrete parse tree therefore is a set of possible derivations that are being uh summarized that would result in the same parse tree by contrast an abstract parse tree does not show how the input was parsed in and does not have non-terminals as interior three nodes but the abstract parse tree only summarizes uh only represents the syntactic structure of the input so for example for uh for the assignment that um for the con that left side the left-hand side the parse tree uh for the concrete parse tree here uh represented as an abstract parse tree would look like this where we have an assignment node that has a left hand side in the right hand side and then for the expressions we have binary operator nodes that happen to be addition operators with the left hand side and the right hand side and the non-terminals are removed as interior nodes and this is essentially uh what the parse trees look like that we are producing for our compiler so we have an assignment expression node we have a binary operator expression note it contains an enumeration constant for the operator and so therefore the parse trip would have uh pretty much exactly this structure here




Ambiguous Grammars (6 min): https://youtu.be/V-26Qm-82zM
so why do we keep writing ambiguous grammars and what types of ambiguity are there the big reason uh the biggest advantage of uh ambiguous grammars is that they make it very convenient to write down the syntax of the grammar for human consumption so for example we could write we could specify expressions as saying an expression is either an identifier or a number or a pair of expressions with a multiplication division addition or subtraction operator or it could we could have an expression in parentheses so with just a few very simple rules we have spelled out the syntax of expressions if it's an additional document or maybe in in an english description we then have the additional information about the associativity and the precedence of the different operators so what does this mean for the parse trees that are generated suppose we have the input one minus two minus three we could have two possible parse trees uh generated uh the left subtree which has uh one minus two uh as a subtree and then the second minus is at the root with the right sub 3 being 3 or the right subtree where the second operator is in the subtree now um which of those parse trees do you prefer maybe pause the video for a moment and think about which which parse tree would be the more desirable one well for deciding uh which parse tree to use let's think about what an interpreter might do for uh for interpreting the input expression once the parse tree has been produced so an interpreter would evaluate the expression by traversing the parse tree so with the left uh tree uh we recurse down the subtrees for the left subtree we first evaluate one minus two which results in minus one and then uh for evaluating the root we have the values of the two sub trees we have minus one and three and then minus one minus three that would would give minus 4 is the result of the left of the left 3. for the right 3 we first evaluate the left subtree that's 1 that's easy then we evaluate the right subtree so we have two minus three is minus one and then back to the root we have one minus minus one which is zero that's not exactly what uh how math works right the reason uh that in math we get minus four instead of zero is that in math uh one minus two minus three is implicitly uh with implicitly has parentheses to the left as an open parent one minus two closing paren minus three because minus is a left associative operator and left associative operator corresponds uh to the left uh tree here now what about if um if the second operator is multiplication so one minus two times three which uh tree structure do we want then again maybe pause the video for a moment think about it well again let's try to evaluate this on the left three we have one minus two which is minus one and then we perform the multiplication minus one times three but result in minus three and the with the right three we have uh two times three which is six and then one minus six would be minus five and that's what math would produce because multiplication has higher precedence than subtraction and for the parse trees what we want is that the higher precedence operator would be further down in the tree so what are the trees that we want to produce if we have a mix of higher precedence and lower precedence operators then the higher precedence operator should be further down in the tree than the lower precedence operator so that means for the expression 1 minus two times three we want the right parse three out of those two for operators with equal precedence the parse tree should reflect the associativity of the operator because a minus is a left associative operator that means one minus two minus three the meaning of that is open paren one minus two closing for n minus three that because of that we want the left uh subtree so that the parse tree reflects the associativity of the operator once we have the trees built correctly once we have the correct structure for the trees parentheses are not needed anymore in fact we don't need to if the parser builds the tree correctly we don't need to record the parentheses in the tree at all in an abstract parse tree because it would be implicit in the structure of the tree so for example the only way we could have gotten that left subtree here with um minus below the the multiplication operator is if the input was open parent one minus two closing paren times three so again parentheses are not do not need to be recorded in an abstract parse tree



Resolving Ambiguity (17 min): https://youtu.be/36y5LIWCorM
so finally how do we get rid of ambiguity how do we resolve uh an ambiguous grammar so that our parser can understand it one solution is uh that we fix the grammar itself to make the grammar unambiguous the for the expression grammar that we had uh a few slides ago in the previous video this is what what we would need to produce so instead of a single non-terminal e for expression with all four operators plus minus times divide we need to break it up into three different non-terminals e for expressions t for terms and f for factor where the lower or the lowest precedence operator is at the top in uh in expression so in expression we have addition and subtraction and then because addition and subtraction has the same precedence level and the grammar rules for addition and subtraction are left recursive because the operators are left associative so we have uh the grammar that says an expression is an expression plus the term or an expression minus a term or just a term by itself if you don't have any uh additional subtraction operator then similarly for a term a term is a term times a factor a term divided by factor or just a factor by itself so the again uh term is left recursive because the operators are left associative and about multiplication division have the same precedence so therefore they are the same non-terminal and they are further down in the trees so that's why uh where a term is inside an expression which results in the multiplicative operators being further down in the tree structure and then finally for the leaves of the three is either an identifier or a number or we could have an expression in parentheses that's the highest precedence the parenthesis operator now uh this is for just two or three precedence level uh levels uh additive operators multiplicative operators in the parentheses in a uh in a full-blown uh programming languages there is a there are a lot more in c or c c c sharp or java there are 15 precedence levels in c plus plus 17 simply google president's levels with the corresponding language and you'll find those tables another common uh source of ambiguity that's what's called the dangling if or the dangling else if you have two nested if statements so the grammar foot for if or if then else would be a statement is if expression then statement else statement or just if expression then statement without an else and just to simplify it under other that summarizes all other statements uh anything other than if so suppose we have two nested ifs one is in if then one is in if then else where does that else belong to does it belong to the inner if or does it belong to the outer if again um think about this for a moment uh pause uh the video and uh and see if you can figure out what it is uh by the way that here this is a this uses a then keyboard but of course in c c plus java it's uh exactly the same thing except that we have parentheses around the condition so uh let's take java you should you should know this language you've been programming in this uh for a while so uh what the uh what do two nested if statements mean in java does the else belong to the interif or the out of all right pause the video and um and come back in a moment all right no matter which language that is if if you have a dangling else problem in the language that the only way to get rid of the dangling else if would be if you only have if than else if there's no if then or if every if go has an end statement like if and and if if you have it dangling else the else always belongs to the if no matter which language um that's the case for c c plus plus java and that's the case for tiger so that means that nesting structure that i had here is exactly what you expect the else belongs to the inner if now for this particular problem uh our parser generator java cup or most uh bottom up parser generators actually have a trick that automatically fixes that graham it automatically removes that ambiguity and and it does it correctly so you don't actually need to fix the grammar to make it unambiguous but if you would want to make the grammar unambiguous here's how we need to distinguish between matched and unmatched statements were matched statements statements in which every if comes with an else so the grammar for match statement is either an if then else or something other than an if statement and an unmatched statement is either if then or in if then else but now for the the then part of an if then else must be a matched statement uh you can you could have an another unmatched statement in the else clause that is if there could be an if then in the else statement but they cannot be an if then in the then part of an outer if because that otherwise the else would need to belong to the inner if so the only way how you could have an outer uh if then else and then if then in the in the then block is if the inner if would be enclosed inside the curly braces or inside begin and end in that language that is if there's some blocking construct around it so this grammar would be unambiguous for um for if statements but as i mentioned on the previous slide lr parsers can automatically repair the ambiguous grammar so you don't actually need to write the grammar rules for if like this now for eryth for the arithmetic operators uh lr parsers also have a cool trick up their sleeve you can still write the ambiguous expression grammar like in the bottom part of this slide an expression is an integer constant or expression plus expression or expression minus expression or expression times expression uh as ambiguous as it gets but then in addition you have president's declarations uh where you declare left and minus to be left associated declare plus and minus to be left associative you declare times and divide to be left associative and the further down a president's declaration is in that list of presidents declarations the higher the precedence so uh i said it wrong left and right have left left associative at times divided left associative and then the president's four times a divide is higher than the president's four plus and minus and if you'd have a comparison operators like equal and non-equal you can declare them as non-associative and ex if there would be an exponentiation operator in the language exponentiation is normally a right associative operator so you can declare this right associative for unary operators it doesn't matter whether they you declare them as non-associative left associative and right associative it has the same meaning if there's only one now the problem we have with miners is that minus is used both as a binary and as a unary operator and the trick to get the president's right for the unary minus operator is that we introduce a dummy token and uh if you remember right we actually need a token declaration for u minus so we introduce that dummy token u minus for unity minus and and give it the highest precedence or at least precedence higher than uh times and divide and then in the grammar rule for the unary minus we have a grammar rule that says we just have the minus the minus token in front of an expression and then with the declaration percent prec u minus we say for this grammar rule use the president's level for u minus instead of the president's level for minus now for for the tiger compiler all you need for fixing the the the ambiguity for arithmetic expressions is all the right precedence declarations but i would still be careful with this because every time you have an ambiguous grammar rule you or when with multiple with multiple operators and multiple possible ambiguities you get a ton of different shift reduce and reduce reduce operators so uh it's probably best to still develop the grammar just if you uh just one or two operators at the time write down the president's declarations make sure that uh that the ambiguities are gone that there are no uh shift reduced or reduced reduce upper conflicts and then move on to the next operator until you have all the operators defined the we will talk more about shift reduced reduce reduce uh in the in a later video but uh ambiguity results in those types of operators those types of parser conflicts now what does non-associative mean so in thai in tiger you cannot write an expression like x minus y minus z because x x less than y less than z because less than is defined as a non-associative operator the parser will prevent you from parsing this expression correctly instead what you need to write for if the math expression if in math you want to say x less than y less than z you need to explicitly spell out x less than y and y less than z in java the parser will actually correctly parse x less than y less than z but then you get a type error that says that the result of x less than y is of type boolean and z is of type int and so you can't compare boolean with an intent so therefore there's a type error so that means the parser treated less than as a left associative operator and left the error reporting up to the type checker why maybe because um because the type error from simulating analysis is a is a better type error than what the parser could report or just so that the parser can uh can treat all comparison operators the same if you have equal that is the equal equal operator in java and x y and z are all booleans then the expression uh s e x equal equal y equal equal z makes uh perfect sense or might make perfect sense at least it's uh it type checks uh correctly in c and c plus plus even if x y and z are integers or floating points for that matter the expression x less than y less than z is perfectly legal but again it does not mean what mathematicians might think it means it means we're comparing x with y the result is a boolean that boolean is represented as either zero or one and then that boolean value zero one is compared against z but um the parts are perfectly uh is perfectly happy and the type checker is happy with it too if you're comparing editors or if the integers are then automatically uh cast or converted to floating points now what about assignment in the c family assignment is an operator you can use assignment operators inside of expressions but the assignment operator is right associative the value of an assignment operator is the same as the value of the right hand side value so for example the expression x equals y equals z is equivalent to x equals open paren y equals z that's what right associative means and implementation wise you assign z to y the result of that assignment expression is the same as the value of z which then gets assigned to x as well in tiger you syntactically you cannot have assignment inside uh expressions so if you want to think of the of assign it as an operator then it would be a non-associative operator but um you don't actually need to write an operator a president's declaration for assignment because uh you could uh write the grammar rules so that assignment would not be allowed inside uh expressions if you want to if a assignment would uh be allowed in uh actually no because everything in tiger is an expression it probably would still be easiest to uh to write the grammar to make assignment a non-associative operator because uh in a language with statements uh instead of expressions uh you could make a assignment a statement and then syntactically it could not occur inside expressions and uh in that case you wouldn't need a president's declaration for assignment entire because everything is is an expression because we don't syntactically distinguish between expressions and statements still uh the the assignment operator is the easiest to handle by just declaring it to be a non-associative



Building Parse Trees (13 min): https://youtu.be/KGxujef2tG4
so finally how do we get rid of ambiguity how do we resolve uh an ambiguous grammar so that our parser can understand it one solution is uh that we fix the grammar itself to make the grammar unambiguous the for the expression grammar that we had uh a few slides ago in the previous video this is what what we would need to produce so instead of a single non-terminal e for expression with all four operators plus minus times divide we need to break it up into three different non-terminals e for expressions t for terms and f for factor where the lower or the lowest precedence operator is at the top in uh in expression so in expression we have addition and subtraction and then because addition and subtraction has the same precedence level and the grammar rules for addition and subtraction are left recursive because the operators are left associative so we have uh the grammar that says an expression is an expression plus the term or an expression minus a term or just a term by itself if you don't have any uh additional subtraction operator then similarly for a term a term is a term times a factor a term divided by factor or just a factor by itself so the again uh term is left recursive because the operators are left associative and about multiplication division have the same precedence so therefore they are the same non-terminal and they are further down in the trees so that's why uh where a term is inside an expression which results in the multiplicative operators being further down in the tree structure and then finally for the leaves of the three is either an identifier or a number or we could have an expression in parentheses that's the highest precedence the parenthesis operator now uh this is for just two or three precedence level uh levels uh additive operators multiplicative operators in the parentheses in a uh in a full-blown uh programming languages there is a there are a lot more in c or c c c sharp or java there are 15 precedence levels in c plus plus 17 simply google president's levels with the corresponding language and you'll find those tables another common uh source of ambiguity that's what's called the dangling if or the dangling else if you have two nested if statements so the grammar foot for if or if then else would be a statement is if expression then statement else statement or just if expression then statement without an else and just to simplify it under other that summarizes all other statements uh anything other than if so suppose we have two nested ifs one is in if then one is in if then else where does that else belong to does it belong to the inner if or does it belong to the outer if again um think about this for a moment uh pause uh the video and uh and see if you can figure out what it is uh by the way that here this is a this uses a then keyboard but of course in c c plus java it's uh exactly the same thing except that we have parentheses around the condition so uh let's take java you should you should know this language you've been programming in this uh for a while so uh what the uh what do two nested if statements mean in java does the else belong to the interif or the out of all right pause the video and um and come back in a moment all right no matter which language that is if if you have a dangling else problem in the language that the only way to get rid of the dangling else if would be if you only have if than else if there's no if then or if every if go has an end statement like if and and if if you have it dangling else the else always belongs to the if no matter which language um that's the case for c c plus plus java and that's the case for tiger so that means that nesting structure that i had here is exactly what you expect the else belongs to the inner if now for this particular problem uh our parser generator java cup or most uh bottom up parser generators actually have a trick that automatically fixes that graham it automatically removes that ambiguity and and it does it correctly so you don't actually need to fix the grammar to make it unambiguous but if you would want to make the grammar unambiguous here's how we need to distinguish between matched and unmatched statements were matched statements statements in which every if comes with an else so the grammar for match statement is either an if then else or something other than an if statement and an unmatched statement is either if then or in if then else but now for the the then part of an if then else must be a matched statement uh you can you could have an another unmatched statement in the else clause that is if there could be an if then in the else statement but they cannot be an if then in the then part of an outer if because that otherwise the else would need to belong to the inner if so the only way how you could have an outer uh if then else and then if then in the in the then block is if the inner if would be enclosed inside the curly braces or inside begin and end in that language that is if there's some blocking construct around it so this grammar would be unambiguous for um for if statements but as i mentioned on the previous slide lr parsers can automatically repair the ambiguous grammar so you don't actually need to write the grammar rules for if like this now for eryth for the arithmetic operators uh lr parsers also have a cool trick up their sleeve you can still write the ambiguous expression grammar like in the bottom part of this slide an expression is an integer constant or expression plus expression or expression minus expression or expression times expression uh as ambiguous as it gets but then in addition you have president's declarations uh where you declare left and minus to be left associated declare plus and minus to be left associative you declare times and divide to be left associative and the further down a president's declaration is in that list of presidents declarations the higher the precedence so uh i said it wrong left and right have left left associative at times divided left associative and then the president's four times a divide is higher than the president's four plus and minus and if you'd have a comparison operators like equal and non-equal you can declare them as non-associative and ex if there would be an exponentiation operator in the language exponentiation is normally a right associative operator so you can declare this right associative for unary operators it doesn't matter whether they you declare them as non-associative left associative and right associative it has the same meaning if there's only one now the problem we have with miners is that minus is used both as a binary and as a unary operator and the trick to get the president's right for the unary minus operator is that we introduce a dummy token and uh if you remember right we actually need a token declaration for u minus so we introduce that dummy token u minus for unity minus and and give it the highest precedence or at least precedence higher than uh times and divide and then in the grammar rule for the unary minus we have a grammar rule that says we just have the minus the minus token in front of an expression and then with the declaration percent prec u minus we say for this grammar rule use the president's level for u minus instead of the president's level for minus now for for the tiger compiler all you need for fixing the the the ambiguity for arithmetic expressions is all the right precedence declarations but i would still be careful with this because every time you have an ambiguous grammar rule you or when with multiple with multiple operators and multiple possible ambiguities you get a ton of different shift reduce and reduce reduce operators so uh it's probably best to still develop the grammar just if you uh just one or two operators at the time write down the president's declarations make sure that uh that the ambiguities are gone that there are no uh shift reduced or reduced reduce upper conflicts and then move on to the next operator until you have all the operators defined the we will talk more about shift reduced reduce reduce uh in the in a later video but uh ambiguity results in those types of operators those types of parser conflicts now what does non-associative mean so in thai in tiger you cannot write an expression like x minus y minus z because x x less than y less than z because less than is defined as a non-associative operator the parser will prevent you from parsing this expression correctly instead what you need to write for if the math expression if in math you want to say x less than y less than z you need to explicitly spell out x less than y and y less than z in java the parser will actually correctly parse x less than y less than z but then you get a type error that says that the result of x less than y is of type boolean and z is of type int and so you can't compare boolean with an intent so therefore there's a type error so that means the parser treated less than as a left associative operator and left the error reporting up to the type checker why maybe because um because the type error from simulating analysis is a is a better type error than what the parser could report or just so that the parser can uh can treat all comparison operators the same if you have equal that is the equal equal operator in java and x y and z are all booleans then the expression uh s e x equal equal y equal equal z makes uh perfect sense or might make perfect sense at least it's uh it type checks uh correctly in c and c plus plus even if x y and z are integers or floating points for that matter the expression x less than y less than z is perfectly legal but again it does not mean what mathematicians might think it means it means we're comparing x with y the result is a boolean that boolean is represented as either zero or one and then that boolean value zero one is compared against z but um the parts are perfectly uh is perfectly happy and the type checker is happy with it too if you're comparing editors or if the integers are then automatically uh cast or converted to floating points now what about assignment in the c family assignment is an operator you can use assignment operators inside of expressions but the assignment operator is right associative the value of an assignment operator is the same as the value of the right hand side value so for example the expression x equals y equals z is equivalent to x equals open paren y equals z that's what right associative means and implementation wise you assign z to y the result of that assignment expression is the same as the value of z which then gets assigned to x as well in tiger you syntactically you cannot have assignment inside uh expressions so if you want to think of the of assign it as an operator then it would be a non-associative operator but um you don't actually need to write an operator a president's declaration for assignment because uh you could uh write the grammar rules so that assignment would not be allowed inside uh expressions if you want to if a assignment would uh be allowed in uh actually no because everything in tiger is an expression it probably would still be easiest to uh to write the grammar to make assignment a non-associative operator because uh in a language with statements uh instead of expressions uh you could make a assignment a statement and then syntactically it could not occur inside expressions and uh in that case you wouldn't need a president's declaration for assignment entire because everything is is an expression because we don't syntactically distinguish between expressions and statements still uh the the assignment operator is the easiest to handle by just declaring it to be a non-associative



LR Parse Engine: https://youtu.be/NJHKkn_Isjo
the parse engine of an ella parser is what's called a push down automaton it essentially consists of a deterministic finite automaton and a stack where the edges of the dfa are labeled with terminals or non-terminals and the state dfa states are stored on the stack and the dfa is applied to the the states on the stack uh a pushdown automaton has five actions although two of them go together one can be a special case of the other so the important ones are shift and reduce that's why it's also called the shift reduced parser so shift implementation wise shifts a state number onto the stack and that's it conceptually we can think of tokens being pushed onto the stack reduce reduces uh by rule k by some reduces by some grammar rule which means pop the right-hand side of the grammar rule of the stack and then it's followed by the go-to operation which means push the left-hand side of that grammar rule onto the stack and accept means we we correctly parsed the input and we accept uh the input as uh as legal according to our grammar and accept is the equivalent of shifting in the file and then finally uh error which means well we we don't accept it something went wrong in the parse so accept is a special case of shift and reduce and go to basically always come in pairs the so a con as i just mentioned conceptually we can think of shift taking tokens from the input and putting them on top of the stack and reduce uh together with go to replaces the right-hand side of the grammar rule on top of the stack by the left-hand side of the grammar rule the actual implementation is that uh that state numbers are pushed uh onto the stack so shift puts a state number onto the stack pop accounts how uh reduced counts how many elements they are on the right inside of the grammar rule pops that many states off the stack and then it looks up in the go to table which that um which state should be put back onto the stack we'll see this in more detail in a little bit now uh before we um before we look at the parse table and the details of how this is implemented let's just try to trace uh what the shift reduce rs looks like and this trace is uh represented in three columns the left column shows what's in the stack the right column shows what's left in the input file and uh and on the right we have the action that we're performing and for now we'll uh will use the conceptual understanding of how the stack works the grammar is that expression grammar that we had i don't know 20 or 30 slides ago and uh for our purposes there are three important grammar rules uh the grammar rule that says a statement is an assignment with id called an equal expression and then an expression could be either an identifier or a number and the input is the assignment a colon equals seven for for the purpose of a shift reduced parse and the file needs to be represented both in the parse tables and in the input and so we represent in the file as that dollar sign so the input is equal to equals 7 followed by dollar and since we will be working on a statement and that statement says it starts with an identifier the first action that we need to perform is shift so we need to shift the identifier from the input onto the stack so now we have the identifier on the stack and then the input we have colon equals seven a dollar sign again we need so as previously said that the reduced operation uh finds the right-hand side of a grammar rule on the stack and replaces it by the left-hand side why don't we at this point replace that identifier that's on stack with the left hand side being an expression and the reason for this is that at this point with the empty stack underneath or with the only the identifier on the stack we could not uh replace the identifier with an expression because once we have the expression on the stack there is no way how we can get from there to a statement to record to recognize the entire input if a statement might start with an expression then that would be a possibility to replace that identifier with an expression then it might depend on what token comes next in the input but but just looking at the identifier on the stack we cannot represent we cannot replace that with the expression so we cannot reduce that so the the only other choice we have is to shift uh call an equal onto the stack and then on the next line we shift uh we shift the number on the stack so now we have identifier colon equal equal 7 according to the number on the stack and we have end of 5 in the input now at this point we can't uh right we can't recognize the entire statement yet the statement wants an expression after the colon equal but at this point we can we can reduce the number to an expression so we pop the number off the stack push expression back onto the stack by reducing according to the ram rule e error num and now we have id column equal e on the stack this time we can reduce according to the grammar rule statement is an identifier called an equal expression and now finally we have the start symbol on the stack and the file or dollar sign in the input and that's when we can accept uh when it when we can accept the input all right let's try the same thing but with state numbers so on the stack i still put the tokens there but now in somewhat smaller font i have the state numbers and the implementation wise really only all we have on the stack are those state numbers and now together with the shift actions i lift a list the state numbers and the reduced actions talk about the grammar rule by number and um these these numbers two four and five are from the original uh grammar that was way earlier in on the slides the first gra grammar rule said a state statement error statement semicolon statement the third one was a print statement and then we had four or five and then a few more expressions the so the empty the empty stack really uh contains uh the start state uh of the automaton on the stack which is state one and now uh with a state one on the stack and um and then an identifier coming in the input we look up in the parse table and find that we need to shift 4 onto the stack now we have the stage 4 on the stack which represents the identifier now we shift state 6 on the stack which represents i t column equal now shifts 10 on the stack which represents um uh id called equal num and what i'm saying here state 10 represents at equal and equal num whichever state you have from the top of the stack summarizes what's below in particular one as long as you're working on an uh on a single statement the only way that state 10 does not just say that you have a number on the stack it uh it says more about the context underneath similarly uh state six says that we are at this point in in an assignment that we have id called an equal on the stack and we expect an expression in the input and um and then we reduced according to grammar rule five then according to grammar rule two and then finally we accept so um this is what the implementation actually does when i ask you uh to to provide a parse trace for example for homework or uh or exams that uh what i mean is uh to the conceptual version you you'll be able to solve all the problems that you encounter on uh on homeworks or exams by working at this level we don't actually need the details of the parse table for for tracing and for understanding how a certain input program is parsed in most cases and those cases where you actually need the parse table i might actually need to construct them i'm not going to ask you that um this is the parts table and we'll look at this in the next video

Parse Table (14 min): https://youtu.be/FmOCrUnYo_s
so in this video we'll look at the details of what um what the parse table looks like and how actions are constructed and uh will be i'll be flipping back and forth between this slide that we discussed in the previous video and the actual parse table so we start in in state one and uh we we see an identifier in the input we look this up in the parse table so first about the format rows or states and then from id to a dollar sign that's what's uh usually called the the parse table and the last three columns that's what's called the go-to table so uh in the parse table the columns are tokens as well as end of file and then they go to table the columns or non non-terminals so we have an identify in the input we in state one under identify we look up uh what the parse table says and it says shift 4 and that's the action that we are performing and we put state 4 onto the stack now we are in state 4. in the input we see colon equal so in state 4 under colon equal we see that it says shift 6. so we put 6 onto the stack now we have a state 6 with a number in the input in state 6 under column number it says shift 10 we put state 10 under onto the stack and now we start with with reductions so in state 10 with end of file in the input what does it say here state 10 down here and the file in the input it says r5 for reduce according to grammar rule 5. looking back here on the last line on the slide grammar rule 5 is id error num we count how many symbols terminals or non-terminals there on the right-hand side of the grammar rule exactly one we pop one element off the stack so we pop state 10 of the stack and underneath we have state 6 left now the left hand side of that grammar rule is e for expression so now in state 6 under expression in the go to table we see the action go to 11. so we push state 11 onto the stack which is the equivalent of putting the left hand side of that grammar namely the non-terminal e onto the stack now again in state 11 with in the file in the input what does the table say state 11 and the file in the input it says reduce according to gram rule 2 gram rule 2 oops gram rule 2 has 3 symbols on the right hand side of the grammar rule so we pop three states uh off the stack we pop 11 6 and 4 of the stack underneath we have state 1 in state 1 under s for the not left inside non-terminal we find the action go to two so we put the state two on onto the stack so now we have one and two on the stack we have end of file in the input what does the table say in state 2 on the end of file it says accept all right now this looks like magic i didn't think i didn't tell you how to construct the table and uh i'm not gonna tell you um while we're working on the parse module at some uh at some point later in the semester we might have time to talk about how the parse table is constructed but at this point that's beyond what we what we need for our purposes for working on that parser as well as for homeworks and exams but i'd like to discuss a few special features of the parse table what you see in in all rows in which they reduce uh operations row 5 state 5 10 and 11 that there is a mix of error actions any any empty table slot is an error action there are a mix of error actions and reduced operations and in state 11 we have a mix of reduce and shift and what this tells us is this is a parse table for a parser that uses one token look ahead because a parser without lookahead and lrc or a parser must decide whether to shift or reduce without looking at the look ahead character that they look ahead token which means that all uh that all table slots in the entire row in the entire uh for in the entire row for any of the tokens must be the same reduced operation so this is a parse table for a parser that uses a lookahead and where the parse table was constructed with lookahead in mind now there are several different parsers with with one token look ahead we'll we'll get to that but at least this is not in lsu reports so the these are just some notes that i mentioned uh previously so the right three color columns are the go to table the rest is the parse table um a dollar means uh indicates end of file and that this this is the parse table for an la la one or ll one or actually maybe an slr uh parser but i'm pretty sure it's and it's l1 and um and the state on top of the [Music] yeah and so what the parser does is in the state that's on top of the stack it looks up the action based on the input token and in lsu that's what an alley l1 parser does and so the lala1 parser always looks at the at the first token and based on that looks in the parse table do we have shift or reduce if it's shift uh put it put the appropriate state on the stack if it's reduced then continue and then ls0 parser needs to decide whether to shift or reduce based on the stack counted alone that means based on the state on top of the stack and if the ls-0 parser decides decides to shift then it gets the token from the input it checks which token it is looks it up in the parse table and puts the appropriate state onto the stack and so if it's if the llc ls zero party reduces the entire row must it must all be the same reduce action now um there are there are few possible parse conflicts that could happen in the table construction algorithm if uh if that table construction results in two different actions in the same table slot then that's a parse conflict if um a shift reduce conflict would be a conflict where you have both the shift and the reduced action in the same table slot and the reduce reduced conflict is a conflict when you have two different reduced actions in the same table slot and sometimes for shift reduced conflicts the the parser generator can automatically resolve the shift reduce conflict in favor of shift uh well it it always results in favor shift but sometimes that automatic resolution is is exactly what's being called for and that fixes the problem for reduced reduced conflicts uh the automated resolution favors the order of uh of grammar rules in the input file and uh that's basically always wrong i can't think of a r uh of the grammar uh where where this would be the the right way to resolve the conflict and uh in the next slide in the next video will um actually know let's um let's continue in this video this is what um what the output would look like if you have a shift-reduced conflict or what what the that's the information that you find in the the java cup uh error file this is a in java cup actually doesn't format it uh this particular way i believe this is the formatting from uh from emiliac which uh but this is how it's presented in the in our textbook the information is the same so here it says in state 17 there's a shift reduced conflict and then it tells us what their shift to lose conflict is we could either shift the else token or reduce uh according to grammar rule four and then it shows the different parser configurations uh a parser configuration is a grammar rule that contains a dot dot or in java cup it's represented as open parent store closing paren on paper it when you construct the parse table you always use a dot where the dot indicates whatever is to the left of the dot is on the stack whatever is to the right of the dot is still in the input so this in state 17 there are two possible parser configurations we obviously look in at at if statements and we could either have an entire if-then statement on the stack and with the dot on the far end of the uh of the grammar rule on the right hand side of the the right the rightmost position of the grammar rule that means that we could reduce according to that grammar rule or we could have an if then else where there's an else coming in the inpu in the input and the else token would be the next token that we read and so um what the shift reduce conflict tells us we could either shift the else or reduce according to grammar rule four and grammar rule four would have been the if then statement instead of the if then else and then underneath it shows uh the parse table row that's that got constructed for state 17 and it says in column else so we we have a shift 19 that is if we see the token else then we shift into rule we shift state 19 onto the stack and dot means anything else anything other than else be reduced according to grammar rule four so what happened here is that shift reduced conflict was automatically uh resolved in favor of shift that is if uh the the look ahead token is else then be immediately shifted and this is exactly what you want that for an if then else that the else goes with the innermost if and you will have the same shift to lose conflict uh for your parser for the project and that's perfectly fine you don't need to change the grammar to make it unambiguous now a again to summarize what uh the the parse actions do shift means we consume a token from the input and we shift the shift the state that represents the token onto the stack reduce k means we count the number of tokens or non-terminals on the right-hand side of rule k pop that many states of the stack and in the state that's left on top of the stack we then look up uh the left-hand side non-terminal of rule k and in the go to table and there we find the go to m instruction that says we now push a state m onto the stack and accept means we stop and report success and error means we stop and would mean we stop in the report error but the parser also has a error recovery mechanism because it's it's undesirable to uh to immediately stop the parts after the first error so after reporting the error the parser goes into error recovery mode to see if it can recover from the error and can continue parsing


R/R Conflicts (13 min): https://youtu.be/uo-6dneUiBA
here's an example of a grammar with a reduced rules conflict we have operators or and equal and plus with president's declarations defined so therefore we shouldn't have any uh reduced reduce or shift to those conflicts based on the operators themselves now in the grammar we have two types of assignment statements an assignment statement with an identifier on the left the assignment operator and an arithmetic expression on the right and another one with a boolean expression on the right syntactically there's a difference between pulling in arithmetic expressions boolean expressions have operators or in an equal and arithmetic expressions have the operator plus so why would this cause a reduced reduced conflict this slide already says that there's a reduced reduced conflict involving the identifiers but why if the input is id assign id or id for example then there's no reduced conflict if the input expression is if the input statement is id assign id plus id again there's no conflict where the conflict occurs is if the right hand side of the assignment is just an identifier followed by end of file there is no operator without operators the parser cannot figure out whether to reduce the identifier to a boolean expression or to an arithmetic expression if the remember parse um parse actions uh occur only in context so for example when the left hand side of the the assignment is on the stack which is also an id then the the grammar rules for reducing the id to boolean expression or arithmetic expression don't apply because the assignment statement does not start with an arithmetic or boolean expression so starting out at the beginning of the statement if there's an identifier it um it's obvious from just the information on the stack that um that this identifier should not be reduced now once we have a sign on the stack and then we see the next identifier then an an lr 0 a parser would always have a conflict because it wouldn't know whether this identifier is should be reduced or not and lr1 parser can look ahead at the next token if the next token is or and or equal then the identifier should be reduced to a boolean expression they identify after the assignment that is if the the look ahead is the operator plus then that identifier should be uh should be reduced to an arithmetic expression so with lookahead we can resolve the problem but if the look ahead is end of file that is if we don't have an operator afterwards then there is a conflict now this is how the parser tells us again this is the the format for indicating a parse conflict that's shown in the book which i believe is what ml yak uses the information is essentially the same that java cup produces just a different formatting so it tells us that there is a reduce reduced conflict between rules six and four on end of file and then afterwards it shows us those rules boolean expression is id and the arithmetic expression is id with the dot and the in the right most position in both in both parser configurations with the dot in the right most position that indicates that there is a reduced up very a reduce action possible and then uh it shows us the row in the parse table for state five on plus be reduced according to grammar rule six on end or uh and or or we reduced according to grammar rule four so it seems that grammar four is the boolean expression grammar rule 6 is the arithmetic expression then an equal be reduced according to grammar rule 6 equal is a boolean expression but the arguments of equal are arithmetic expressions and then it says and end of file be reduced according to grammar rule 4. so this is the automated conflict resolution by java cup that says if there is uh if if there's a reduced reduced conflict pick whichever grammar rule occurred first in the input file and that grammar rule was uh reducing the identifier to boolean expression and then for any other look ahead there is an error now is this automated conflict resolution useful in this case certainly not because it means that we could not have an assignment uh in with with an uh arithmetic identifier on the left-hand side in on the right-hand side that that language construct would simply be outlawed would not be reachable with the produced parser the i can't imagine uh any any example where that automatic resolution for reduce reduced conflict uh would make sense and would be safe i think this is just a a way for the parser generator to spit out some code in order to start testing but in my opinion reduced reduced conflict is always a problem and it has to be resolved um i i haven't tried to construct an example where where this is what you would like to have that the automated resolution works but uh unlike with shift reduce where like with the dangling else where resolving in favor shift makes sense with reduced reduce it just doesn't make sense so now uh what this means we can't rely on the automated resolution we need to change the grammar what are uh some of the possibilities for uh for operators and uh and expressions the obvious possibility is to push the decision to semantic analysis so instead of having two different assignments with arithmetic expressions or boolean expressions in the right hand side we just have one assignment with an expression on the right hand side where the grammar for the expression says we could arbitrarily mix all of the operators or and equal and plus so therefore the parser would allow a nonsensical expressions such as a plus b a and c that's because the grammar says all those operators could be mixed and then the semantic analysis that is the type checker needs to look up the the types of identifiers in the symbol table and based on that check whether those those expressions are type correct and then it would catch the type error so for any problems where there is co where context information is needed to resolve the issue that context information can be processed in semantic analysis and the job of the parser would be to accept a superset of the language in this case to accept uh expressions such as a plus b and c uh so that then uh semantic analysis can further narrow down and decide whether whether that parse tree is type correct or not another solution was uh chosen in um in g plus plus when i was working on the compiler for my dissertation research that was at the time where between versions 2.6 and uh 2.8 the uh since then uh cheapest plus gary karinus a new parser so i don't know whether uh this still is an issue in achievers plus but um no matter how it's implemented uh this uh c plus statement here is ambiguous c x uh parenthesis a comma b comma c closing paren semicolon if a b and c are types like if they are class types or um type def uh type aliases then this is the forward declaration of a function x with parameter types a b and c but without function body so that then that the type checker would know about the function that then another function could call x and then later you'd have another declaration for for x with the function body if a b and c are variables then this is a constructor call we are creating an object of class c the the name of the object is x and we're calling the constructor and pass the variables a b and c to the constructor so presumably in the grammar uh function declaration and a constructor call would be different non-terminal so we need to resolve this which would probably be somewhere along the way there might be a reduced reduced conflict so how could that be resolved the the design decision in g plus plus at least at the time again i don't know whether it's still the case is that the decision was pushed to lexical analysis that the lexical analyzer was required to distinguish between three different identifiers regular identifiers that is variables type identifiers like class names or type aliases from typedef and label identifiers labels are easy to distinguish they're either just before a colon if there's a label on the left inside of a statement or right after a gold tool but between type identifiers in some cases are easy to recognize if there is a variable right after the keyword class or struct then that's a type identifier but to make it more general what what they did is they took semantic information from type checking and fed it back to um to lexical analysis and the reason this worked is because this was a single pass compiler where the parser contained the calls for type checking bits and pieces of code so the type information was fed back to to the scanner and the scanner when saw an identifier would look it up in the symbol table to see have you seen this identifier before do we know what type of identifier it is and um if that didn't help if you haven't seen the identifier before then the scanner would read reading more tokens into a buffer and look ahead in the token stream and figure out with fancy analysis on that on the future token stream whether the identifier we're seeing right here would be an identity type ident so very complex interaction between scan and parser just so that the parser could be made simpler of course another possibility would have been like before not have the parser distinguish between uh object creation and the uh and four declaration and let the later analysis figure that out



S/R Conflicts (19 min): https://youtu.be/0g07SgBEswU
what are some of the ways to resolve a shift reduce conflicts uh we've previously seen that uh we'll have a shift reduce conflict for uh if if then versus if then else with a dangling else problem if the language construct does not have an uh and if uh keyboard the and that uh one possibility there would be to restructure the grammar or we could just rely on the automated resolution by by the parser generator to give preference to shift over reduce another place where you'll see a lot of shift reduce conflicts and reduce reduced conflicts is for uh for arithmetic expressions and the easiest way to get rid of them is with president's declarations for any binary operators define the president they have a president's declaration with uh either left or right associativity uh for comparison operators or assignment if assignment would be an operator we might have non-associated as uh associativity for unreap and um the further down in the list of declarations the higher the president's so in this nonsensical actually i just said for a sign we could have non-stock that depends on the language in c uh or other languages derived from c assignments would be right associative in pascal for example it would be non-associative and uh the further down in the list uh of this pre these president's declarations are they hired the president so this here says that assignment has higher presidents than plus and equal has higher precedence than the sign and this here is just um examples of individual declarations but the president's order obviously does not make sense uh what would make sense would be to have for plus to have higher precedence than the sign and equal between a sign and equal that uh probably could work this way and uh for unary uh operators it doesn't matter whether you assign whether you define them as left associative right associative or non-associative but the way you do have an issue is if if you want to have the same the same operator both as a unary and as a binary operator so for example uh unitary minus versus binary minus and the trick here is to define a dummy token let's call it u minus provide a precedence for u minus which would be higher than all the binary operators and then in the grammar rule for for the unreminders we have the minus operator followed by an expression and then the declaration percent correct u minus to tell a java cup use the president's level of u miners instead of the president's level of miners another shift-reduced conflict that has to be resolved that's what we have in tiger for array creation expressions versus array reference expressions so under the non-terminal x we would have an expression could be a variable or an array a creation expression the syntax for an array creation is id which is a type name the the array element type then between left and right bracket we have the size of the array as an expression and then after the keyword off we have an initialization expression so that all the array elements get initialized to that value now in turn for variables a variable could be a single identifier or it could be a variable with followed by uh an array index in uh in the uh left and right brackets between left and right brackets or it could be a variable followed by doubt followed by another variable but for the purpose of this example here we don't need this so why is there a a conflict suppose we have uh id on the stack uh both both var and x so an expression can start with a bar a var can start with an id or an expression can start with an id for the beginning of an array a creation expression so with just the identifier on the stack we could uh and the left bracket coming in the input we could reduce the identifier to a bar according to the first grammar rule for bar and then with var on the stack we continue reading left bracket expression right bracket and work on an array a reference expression and then eventually once we have the array reference expression we might reduce that to a bar and then the bar to an expression or we skip reducing the identifier to a bar immediately push the left bracket and then we are working on the array uh creation expression so if um if we let the parser generator fix that automatically for us it will give preference to shift and having preference to shift means we always we'd always be working on an array a creation expression and we never could have an array reference expression what we could have is something like x dot y left bracket 5 right bracket if um if we first have a field expression then uh and then uh an array index between brackets but we could not have and identify a fault by left bracket as an array reference expression now what's the solution for for resolving this the trick is to make sure that for inside var you don't have a bar in front of albrecht that you might have id in front of albrecht or something else in front of albrecht but not far in front of albrecht and so the trick for achieving this is simply to unroll the the grammar of war under all the recursion once so that uh that then you have three different array reference expressions one with an id in front of the outbreak and then one with an expression that involves another bracket and one with an expression that revolves a dot in front of the outbreak and then that then if there is just an id in front of the outbreak we would not reduce the id to a bar instead we would uh continue shifting and then once we have the entire once we have id outbreak expression or break on the stack then after that we have the parts decision whether we we were working on a var or whether we were working on an array reference expression if after the arb wrecked we have uh token off then it was an array creation expression if there's anything other than off it was an array reference expression so here the trick is simply to unroll the recursion another uh parsing issue occurs with uh declarations in um the the grammar in tiger says in many places we can have zero more declarations and so here is a random grammar a pair of grammar rules for zero more declarations that happens to be uh left recursive here and then a single declaration could be the variable declaration function declaration or type declaration now for tree construction inside decals plural you'd have the tree construction for creating absentee nodes for linking declarations together with declaration list notes but for consecutive function declarations and consecutive type declarations we should not link them with decklist nodes instead inside abstinent function deck and inside absentee type deck there are next fields that points to other function declarations or type declarations and the reason for that is for any consecutive function declarations they can be mutually recursive any consecutive type declarations can be mutually recursive but if you have a function declaration followed by say a variable declaration followed by another function declaration then those two functions cannot be mutually recursive and so for whichever consecutive function declarations we have we want to connect them with the next field whenever we have consecutive type declarations we want to connect them with the next field that means in the grammar uh for function tackle and type decal we need to make them plural that we have uh one or more function declarations or one or more type declarations in that inside fund fund decals and type decals that's where we connect the declarations using absence of function deck and absent type deck and that obviously makes the grammar ambiguous because based on how we parse it we could either connect consecutive function declarations with the next field inside a function tech node or using a decklist node now the it's relatively easy to write this grammar in a style so you have a reduced reduced conflict that's bad news as we discussed uh before so the trick here is to write the grammar such that uh first of all such that the grammar is structured so you can conveniently conveniently connect them using the decklist nodes or function deck or type deck nodes so for example do you want decals to be left recursive or maybe should it be right recursive and how do you use how do you write fund decals and type decals so that you can have convenient constructor calls for function deck and type deck and then write this in a way so you have shift reduce conflicts that are resolved in favor of shift where shift means that we keep working on fun decals and tight decals respectively instead of reducing the fundamental tackles to a decal or the type decals to a deco so this will require playing around with the grammar and uh getting it into the format so that both the tree building works and that um that he can make have the right constructor calls and that you're connecting uh consecutive function declarations and consecutive type declarations correctly here is one more example for shift reduce conflict this is for a java subset i ran across this for a a compiler i i a a student compiler taught in the undergrad compiler course at purdue ages ago when i was grad student there and here the grammar says a statement can be a variable declaration or an assignment the variable declaration is of the form type of followed by identifier followed by semicolon so regular type declaration in java and an assignment is an l value followed by an equal operator an expression and a semicolon an l value is um an expression that occur can occur on the left hand side of an assignment essentially l for left hand side of the assignment there are other issues with l values for code generation that we'll be disc uh discussing when working on project five and um basically for an l value we need a memory location where to put the value in and if the same expression occurs on the right hand side of an assignment we need to fetch uh the value from that memory location now a type could be a qualified name something like a packagename.packagename.class name for example or it could be a built-in type and an lvalue again could be a qualified name for example class name dot field name or an l value with uh with brackets or an l value with a dot and another identifier and the qualified name is simply uh one or more ids with dots in between so what here is the shift reduce conflict may uh maybe pause the video for a moment and think about this okay the the reason for the shift reduce conflict has to do with the two dots in l value and a qualified name so if we have an id on the stack and start in the input we could reduce the id well first of all we reduce the id to qualified name now we have qualified name on the stack and starting the input and now we could either shift the dot and continue working in qualified name or reduce qualified name to l value and then with l value on the dot on the stack shift the dot afterwards and work on an l value in this case um how you built the tree might have ramifications for further processing and semantic analysis when processing a qualified name we need to do things like like process uh nested package directories loading loading classes from packages etc until we have a class that we that we want to work with while uh for l values uh normally that's just field dereference uh accessing a field inside an object for example and so the for further processing what would be most desirable would be to stay to stick with qualified name uh as long as we can so that um that we we then built the tree with qualified names for enabling the package lookup mechanism and semantic analysis and then once uh we we have something other than a dot coming afterwards then we reduce qualified name to an l value and then continue working in with l values also if if we um let's see if we have qualified name on um on top of the stack and we reduce the qualified name immediately to an l value then we could not reduce the qualified name to a type so it we would have a reduce reduced conflict potentially here as well whether we have a whether the qualified name is reduced to an l value or type but this uh reduced rules conflict could be resolved with a look ahead if there's a left bracket or dot coming afterwards it would we would reduce to a qualified name and if there's an identifier coming afterwards we would reduce uh to a type so um again to to summarize uh a qualified name could be something like java.util.hash table and l value could be something like x dot y dot z and we might have different processing for the uh for the different for qualified names versus l values and the solutions in this case uh i i'd need to try this out again and uh and play with the grammar see what uh what java cup says but at first glance it looks like giving preference to shift might actually be uh what solves it what i did at the time i added uh president's declarations i defined the albrecht and topped as operators and gave adult higher presidents than albrecht and the uh and i believe both albrecht and id and dot had higher precedences than all the arithmetic operators or comparison operators and that solved the problem as well so um you can you can use the president's declarations for something other than um than arithmetic operators but if you do as in this example you might even be able to use it for for if they have if and else as two different operators but if you do that you need to be very careful uh how uh how the operators other than arithmetic operators that you define play together with the arithmetic operators that uh ex that are operators in quotes like albrecht or dot don't interfere with the parsing of arithmetic expressions so this is the conf the content on um on reduce reduce and shift reduce conflicts next we'll have a look at the at error recovery


Error Recovery (14 min): https://youtu.be/kem2YrRMcYI
one of the disadvantages of using an lr parser at least with most parser generators is that error reporting is very simplistic in the parse table there are a lot of empty entries and each empty entry is is an error so what does the parser do if it finds uh in an error slot in the parse table it simply says syntax error and then by default it gives up parsing that means it after the first error by default it simply stops so there are two problems with that first of all the error message syntax error is not exactly very illuminating and second we don't want the parser to stop after reporting a single error we wanted uh we want to continue parsing the input file so that we could find either additional parse errors or that we then could take the the incorrectly parsed program or this the abstract syntax tree and try let the type checker produce more error messages so that the programmer gets gets more errors to work on so the goal is we want to recover from an error and um and continue parsing as if nothing happened and then also maybe uh produce better error messages so let's uh look at error recovery first the bottom-up part generators have this special error token that when you use the error token in in a grammar rule then that triggers error recovery so how does this work wherever an error token is used or if an error if if the parser finds an error if it reports a syntax error then it uh finds the power state uh corresponding to an error token that would uh that might match the input or that would uh that would apply and then it removes uh tokens on the on this on the stack or they remove symbols on the stack it pops the stack until it finds a position where it finds the error token then it shifts the error token then it decodes discards input input tokens from the input stream until it can continue parsing and then it resumes parsing so that means until it is in a state in which there's a non-error action and uh when using these error tokens oftentimes the easiest way to use this is to have what's called the synchronizing token afterwards we'll see this uh in an example in the next slide and you need to be careful with introducing error rules or error productions uh because they very easily can lead to a bunch of additional shift reduce or reduce reduce conflicts and that that would be undesirable so here's a simple grammar an expression is an id or an expression plus an expression or an expression in parentheses or that we could have an error in between the parentheses that is if um in between the parenthesis there is erroneous input then that error token could be used for capturing that around this input in between parentheses we might also have a list of um of expressions expressions plural which is either a single expression or uh multiple expressions followed by semicolon followed by another expression and then there is another error production that says there could be an error followed by a semicolon followed by another expression so how are these used suppose the input was open parent x plus plus y closing parent so the error is that we have one too many pluses or that we might have a missing variable in between or in between the pluses so what happens when we try to parse this input everything goes fine uh as uh with the first three tokens open paren x and plus we shift all of that onto the stack and now we see the plus in the input with plus on the stack plus in the input there would be no way that this could be uh legal so so therefore uh the parser reports a syntax error the error message simply says syntax error that's it now uh we try to recover from this by deleting tokens from the stack we delete the plus there still can shift the error token we delete the identifier that is x from the stack now with just the left paren of the stack at this point we can shift the error token because of that the fourth grammar rule on this slide here so after an open parent we can shift the error token then we remove uh tokens from the input until we can continue parsing so with the error token of the stack plus in the input we can't continue parsing with the error token and uh y on the step in the input we can't continue parsing either but if we remove both plus and the identifier y then with the right hand uh with the right parenthesis in the input then we can continue parsing normally so at this point we can shift that right paren and uh pretend nothing happens and then in the next step uh left paren error right paren would be reduced to an expression and the parser would continue operating from here on now what's the idea of a synchronizing token when you have an error or when programmers have syntax errors in their code they very rarely have multiple syntax errors back-to-back so if there is a syntax error there is no problem throwing away the entire statement it's not necessary to to continue parsing with the next operator for example so useful synchronizing tokens are tokens that occur in in regular intervals in the input file so that we can throw everything away until that next synchronizing token on the previous slide the the last grammar rule sema colon is a very good synchronizing token that means throw the entire expression or the entire an entire statement away until we see the next semicolon then we continue parsing uh a closing paren may be a useful synchronizing token if it's not used uh indiscriminately so um for example if the if the parentheses don't match and if this is uh in the middle of an expression with a lots with lots of nested parentheses then the parenthesis may not be such a great synchronizing token um another good synchronizing token would be a closing a curly brace if you have uh say throw an entire function body away or for a let expression uh entire the the end or the in keywords so by synchronizing on in we would throw all the declarations away by synchronizing on the end we would throw the left body away so those would those are typically good examples for uh synchronization if you have if you don't have a token after the error then there is a danger that this could introduce a shift reduce or reduce reduce conflict but not always sometimes it works depending on what's in the follow set of that non-terminal so again don't use error productions too frequently the reference implementation has maybe about five or six not not more than that now what about better error messages uh just saying syntax error is not a very useful error message so one possibility would be to to leave some types of errors up to semantic analysis so for example in our compiler the break statement can only occur inside of a loop but that is inside a for loop or inside a while loop but if you report a syntax error if the break statement occurs outside of the loop it'll be difficult for a programmer to figure out why is their syntax error because syntactically the statement by itself looks correct enforcing that break statements only occur inside loops that would be easy to do for for the parser but it may be preferable to simply accept the break statement leave it up to simulating analysis to have a more meaningful error message another possibility would be to have special grammar rules for error reporting so for example this is an a c style array variable declaration with an initializer so we have the type that's the array element type the variable then in between left and right bracket we have the size of the array an assignment an assignment operator and then between left curly brace and right curly brace we have we have an expression for in this case just a single expression for initializing a single for initializing a variable or we could have multiple expressions uh inside like a list of expressions followed by essay mccollum and then the appropriate code for producing the power stream suppose that a common error that programmers might make is that there would be an empty initializer it's okay to leave off the initializer but an empty initializer is incorrect so then one possibility would be to have an explicit grammar rule with an mp initializer and then in the simulated action for that grammar rule report an error now in this case it may not be necessary to trigger error recovery because we already recognize the entire statement all the way up to the semicolon all we need to do is report the error the parser would continue parsing uh semantic analysis would would continue and then after semantic analysis main is going to check every scene and error yes there was an error so that's not going to generate code but let's stop after semantic analysis yak and bison also have a mechanism that inside these semantic actions like after reporting an error you could also trigger error recovery we're using a special statement that says pretend that just had a regular syntax error and trigger error recovery um it's something like uh all uppercase syntax errors something relatively uh simple i looked in the in the java cup documentation i did not find that when knowing the internals of java cup it may be that this actually exists that this would be possible but it doesn't seem to be uh documented so these are the options that we have with a um with a with a standard bottom-up parser generator like lex like a yak bison or um or java cup next we'll see how we could do better with the bottom up parser generator




Global Error Repair (10 min): https://youtu.be/KM-6vWUa-rE
what would be a nice feature for a parser to have is if it could automatically repair an error and then in addition to the message the syntax error tell the program how the error was repaired so that that might produce better uh better information about what the error was and that would and the the ideal feature for that would be global error repair so let's uh see in an example how this could work suppose we have this tiger tiger array type declaration or this tiger declaration and suppose this is a standard tiger not a c tiger where uh we have let's type a and then colon equal and then in array brackets 10 of 0. so there is an error here if if they have a type declaration with the keyword type then they can't be an equal and equal so something's wrong here with an error production uh like with the error token what we would end up doing is throw away the entire declaration everything from type to zero all of that would be uh would be removed and then we synchronize on the semicolon another possibility would be to have local error repair where we recognize the error that's when we see the colon equal we push uh let's type and the identifier a onto the stack so now with the the information that we have on the stack we are working on a type declaration but now seeing the colon equal that that doesn't fit so we could have a local error repair replace the colon equal with an equal sign and then continue parsing we push equal on to the we report the error report how we replaced it we push equal on to the stack continue parsing and then we see off and we have another error turns out this uh appears as if this was a variable declaration that is we wanted to declare a variable a and initialize that with an empty array but in that case the real error would be that instead of the keyword type we would have the variable bar and that's what global error repair would do it looks uh within the last few tokens that we've seen whether within those last few tokens we could fix an error so that we can continue parsing for a little while longer and this is the burke fisher error repair algorithm it considers a single token insertions deletions or substitutions within the window of the last k tokens and typically k might be something like 15 and it finds the repair that gets us that allows us to continue parsing the farthest and uh that's another window r that we that we can parse for another r tokens and r would typically be a lot smaller something like maybe four and the way it works if you look it up in the textbook there's a nice figure for this but that was uh too difficult to uh to paint this in uh in powerpoint so let me just explain the idea behind it what we need to do is we need to have the ability to back up parsing by k tokens and that requires a more complex data structure than just a single stack and so the idea is we have two stacks the old stack and the current stack and the distance between them is k tokens so the old stack is everything contains everything that's further away than the k tokens that where we consider repairing things and then we have that q of k tokens and we have the current stack where on the current stack we we perform the parse actions as if this was the regular stack but if we find an error then the current stack would be uh would be undone be back up uh research in that queue of tokens whether uh whether we can will be whether we can fix the error that is we reprocess that entire token queue and look for uh for possible fixes that then would let us continue parsing for another r tokens now how expensive is this uh the explanation here sounds pretty complicated sounds like it might be expensive turns out it's actually not that uh computational and not that expensive it's a complex data structure but computationally not that expensive so if we have a fixed size window k and a maximum of n tokens then we have k possible deletions n times k possible insertions and n times k possible substitutions so it's linear not a big deal and since errors don't happen that frequently that's that's the cost we could pay in exchange regular parsing is a little bit slower because the more complex data structure needs to be maintained but it's it would be nice to have a better error recording from the parser now i've worked with the parser generator that that allows that and that was emiliac the bottom-up part of the generator for the functional language ml and the uh so um without doing much it automatically starts uh starts fixing errors if you have a syntax error it tells you what the problem is it is nice it is much friendlier than just saying syntax error unfortunately we don't have that in uh java cup or in uh in bison as well unless it was added to bison uh within the last couple decades since i had used bison um so how does this work in mla for any tokens that contain a value you need to tell the the parser generator if that token needs to be inserted what value should we provide so for example we might say that if you need to insert an identifier let's call the identifier bogus if you need to insert an integer insert the end of the integer one so that for example you don't divide by zero you could also pick maybe 42 whatever uh whatever fits but um one would probably be a relatively safe bet for other things not to go wrong and if it if you need to insert a string maybe insert the empty string or maybe again a string with some marker that indicates that this is erroneous input and then in addition uh as a compiler writer you can specify a certain insertions deletions or substitutions that that programmers might often make so for example in a language like uh tiger where we have colon equal for the assignment operator a very common error is that we use a single equal sign instead of cool and equal or in either way in the language like c or java that we use or mostly the other way that for comparisons we might use an equal sign instead of equal equal so we could we could specify the substitutions replace equal with the assignment operator or replace the assignment operator with the equal operator or again a language like a tiger where semicolon is a is a separator between expressions instead of a terminator in in languages derived from c semicolon is a statement terminator in languages derived from from algol pascal etc semicolon is a separator so be right before an else for example you'd never have a semicolon a lot of programmers put this there there anyway so you might have a substitution that says and this is a two token substitution which works for user defined substitutions replace semicolon followed by else by just an else or insert an empty let body in or a dummy let body in followed by an integer followed by ends if for some reason the body of the let uh expression was missing so those are the types of things that you can do in the ml in ml yang



LR Parser Technologies (9 min): https://youtu.be/ARUaj4rKEdE
we're nearly done with bottom up parsing let's just have this one page a summary of the different parts of technologies that are out there to better understand the different parts of technologies what we would need to do is uh has a closer look at how the parse table construction algorithm works both for ls zero parsing and ll1 parsing once you've how the parse tables are constructed then all the rest is fits better together but this is too much material for for right now while we're working on uh on the parser uh typically in this course after semantic analysis or or maybe so so that is towards the end of project three or maybe even towards the end of project four there there often is a little bit of slack where we can fit in uh some material on uh on constructing parsers but for now let's just a little bit more abstractly compared the different parts of technologies so in the slide so far we've been assuming that we have one token look ahead but that's not necessarily the case in lr0 parser has no look ahead tokens and what an ls euro parser does is it needs to decide whether to shift or reduce by only looking at the stack without looking at the input if we decide to shift then we look at the input see which token it is and then figure out what to push onto the stack now uh this means that in the parse table construction we don't consider look ahead and at runtime the parser doesn't use look at the next thing is an slr parser an slr parser uses the ls0 parse tape construction algorithm that is in the parts table construction we don't consider look ahead but then at the runtime the ls the slr parser does use look ahead and the way this works is we construct the follow set for each non-terminal that we'll see in more detail what the follow set is when we talk about ll or top down parsing a few slides later very briefly the follow set is the set of tokens that can follow a non-terminal in any derivation and um what an ls0 parser does is no matter what the look ahead token is uh if we are in in the state in which we reduce then regardless of the look ahead token we always reduce that is the entire row in the parse table is a reduced action the slr parser constructs the parse table like analysis like for an ls0 parser but then it checks whether the token is in the follow set of the noun terminal that we want to reduce to and only if it is then we actually reduce if it isn't we remove that reduce instructions from the parts table that's a very simple uh trick to to add look ahead to the simple uh lsu repairs table let's skip la la one for now la la one that's actually what uh what most uh bottom-up parts of generators produce that's what yak bison or java cup produce or ameliac for that matter let's first talk about lr1 parsing lf1 parsing uses lookahead in the tail construction where in constructing the states we consider what tokens might follow after that right inside of the grammar rule and um and then at runtime that look ahead information is used as well now the difference between an ll1 and an ll1 parser is the ella the la la one parser has basically compressed parse tables if two states have the same parser configurations except that the look ahead might be different in that case those two states are combined into a single state now a parser configuration uh what we've seen on previous slides is that a parser configuration is a grammar rule that somewhere has a dot that's actually an ls0 parser configuration and ll1 parts are config configuration is the same thing but afterwards we also have a set of possible look-ahead tokens and uh if you look at the the output of uh java cup in the the grand.cut file you'll see those sets of uh look-ahead tokens for the look-ahead information of the uh the lr1 parser configurations and so if two power states have the same uh the same information uh except for the for the look ahead then those two states are combined and that combining could result in reduced reduced conflicts it cannot result in shift-reduced conflict so an la-la-1 parser could have reduced reduced conflicts that an lr1 parser would not have but in practice it results in so much of a um of a compression in so much of the size reduction of the tables especially for real size programming languages that that it's worth it at least it used to be worth it when that parts technology was developed in the early 1970s when machines were a lot smaller now we have a fairly large amount of memory so it would be possible to have full-blown ll1 parsers but it's also not really all that important uh it doesn't buy as much the um though those uh grammar issues can usually be uh fairly easily fixed uh to turn an ll1 uh grammar into an la la one grammar so in practice uh lela one is uh is good enough for most users the uh for uh l1 might be more interesting for um for theoretical purposes but for practical purposes uh what the compiler community decided was that laela one is basically uh where it's at now if you look at a table of uh parser configure of parser generators like on wikipedia there is a there's a list of i think over a hundred part parser generators that are out there uh there's all kinds of parts of generators there's ll1 there's glr there is uh uh parts of parses that have uh completely different types of grammars or whether it allow ambiguity like earliest parser or or glr parser so there's lots more out there but the ones that are uh most commonly used or either ll1 or llk an llk parser is a part is a top-down parser with arbitrary look at and there are also parts generators out there for that so this is the material on lr parsing now we have a few more slides on top down or ll parsing



LL Parsing (12 min): https://youtu.be/nohoT5v9ZMI
the simplest type of top-down parser is a handwritten recursive descent parser and the structure for that is very simple the so here is the structure of the written in c or java the structure of a recursive descent parse function for statements that recognizes in an if and else statement here so what we see here is for corresponding to every non-terminal there would be a parse function that recognizes that non-terminal and so we start we start by calling as the variable toke already contains the look ahead token so uh before we started we before we called s we needed to make sure that the look ahead token is in this variable talk so talk might be maybe a field in the surrounding parser class if this is java code and then we have an if statement if this was an if a case statement if this was an if token then we eat that if uh we consume that if token then we call uh recursively called the parser for expressions we consume the then token the call uh the are further than part we consume the else token we call it parts again for the uh we call the parse function s again for the else part and then we break out we correctly uh recognize the the statement so this is the general structure so what's the um how does this relates to the grammar so the code structure very closely mirrors the grammar structure if you have concatenation in the grammar rule multiple symbols left to right then in code that's simply straight line code like one statement after another what we saw for for this if statement if you have alternation in the grammar a vertical bar or multiple gram rules with the same left hand side then in code you have an if not statement or an if adults chain or a switch statement this structure that we saw in the previous slide assumed that we have one token look ahead for a parser with one token look ahead that variable toke must contain the look ahead token before any of the parse functions is called and each parse function before it returns must make sure that it calls uh that it it must make sure that the look ahead token is in that variable talk before it returns so if uh in this example here for the if then else the last thing that we called was s and that that that call to s needed to make sure that the look ahead token is available if we have a statement where at the very end we we have a token then after eating that token after consuming that token we would need to uh to explicitly get the next token from the lexical analyzer and so this function eat here is is designed to to check whether whether the look ahead token in this variable talk equals whatever is in the argument of each and then it gets the next token so this is for presentation purposes that it would be easy to uh to explain the structure that's how it's presented in the book um if the token is not just an enumeration constant but a more complex data structure that e function might get a little bit more uh complicated although it would still be uh possible to write like this and um in some cases you might need to you might have uh manual calls for getting tokens from the lexical analyzer in [Music] if you took the programming languages course with me we wrote a recursive descent parser by hand that was a mix between one token look ahead and zero tokens look ahead because that was uh that parser was used interactively and uh when an expression is entered interactively we don't want the parser to look ahead at the next expression but once it sees the end of one expression process it evaluate it print the result before moving on to the next expression that that's the only real use a real reason why you might want to have a parser without a look ahead if you are writing a positive processes in the entire file you typically write a parser with one token look ahead so uh the java parser or the um the current uh achievers plus parser those are both recursively same parses with one token look ahead now how does a parse reconstruction work the each parse function would return a parse tree so if we look back here instead of that function s having returned type void the return type would be the tree node type for whichever parse tree is a statement something like absent dot statement and then each one of the parse function calls to e and s would return uh with return the parse trees for the subtrees and then just before the break you'd have the constructor call for or either before the break or uh before the end of the of the function s you'd have a constructor call that constructs a parse tree in the return set or maybe before the break could uh have the constructor call and then return it at the very end of uh of the parse function s now what are the problems with uh top-down parsing the biggest problem is left recursion left recursion just does not work think about translating this grammar rule into code so you have an if statement that checks that reads the look ahead token checks what it is if it's a little z we could parse it as a single z uh what would be the look ahead token for us to parse it as the first grammar rule that is a capital x followed by y well that capital x starts with a little c again so that means by just looking at the little z we don't know whether it's uh the recursive call or whether it's the end of the recursion and um there is no good way in general to uh for a party generator to figure this out so most uh top-down parts of generators can't handle left recursion if you're writing a handwritten recursive descent parser you can't handle electric version that means anything with left recursion must be manually translated into right recursion and if we then want to build the parser corresponding to uh to a left associative operator then we need to have more complex uh parse reconstruction so that the parser is built uh to be that the parse tree is built to be left dissociative even though we don't have a left recursion now there's one exception the antler part parser generate a generator can actually translate the left recursion into write recursion but i don't know how it does that i haven't that's just the latest version of uh anther and i haven't seen this before with with top-down parser generators another problem is common left factors if you have multiple grammar rules that start with the same token say x is in a followed by y when a followed by z if you're just looking at one token look ahead then you don't know whether you should parse it as a y or whether you should parse it as a z one possibility might be to have more tokens look ahead but then it's not in the ll1 parts anymore and that's difficult to do for a handwritten parser so the solution would be to left factor the grammar and the way this would work is that um the part that's different after the a would become a separate non-terminal so you have something like x is a little a followed by rest whereas the rest is either y or z and the idea for this is very similar to math if you have an expression a y plus a z and you left factor that it means you take the common factor a and factor and move it out to the left of the parentheses and um now if these uh if these uh or math expressions uh that have to do with uh integers or floating point numbers it wouldn't matter whether the left right with a right factor it if those are matrices then it matters in which direction the common left the common factor is pulled out and uh this is actually the exact same operation that you have with grammars uh concatenation in uh in math expressions corresponds to concatenation in the grammar and the plus in math or math expressions corresponds to concatenation in the grammar and plus corresponds to the vertical bar to uh to the choice operator there are other issues in translating grammar into code so the parser must choose the look aheads must choose between multiple alternatives based on a look-ahead token but if the different alternatives are non-terminals then we don't know what the look-ahead token is we need to find out what the possible look-ahead tokens would be if we have something like on the previous slide where we just have a little a there then we know what that look at open is but in this example and x is a capital y or capital z we need to compute all the sets of terminals with which a y can start and all the sets of terminals with which a z can start and then we have correspondingly more complex uh if statements for uh for writing the test whether we recognize this as a y or as a z another related issue is if you have an empty right inside so for example x error empty or x x is a y in this case we need to we need to compute all the terminals with which a y can start and we need to know all the terminals that can follow in x that means what are the valid terminals uh for which we should choose the empty right-hand side and to do this uh for an arbitrary grammar what we need to do is compute the what's called the first and the follow sets for e first for each non-terminal and then for each grammar rule and we'll do this in the next video



LL Parse Table Construction (16 min): https://youtu.be/Q7AWHv2SIW0
for constructing a parser for an arbitrary grammar there are three bits of information that we need to know uh first for each noun terminal nullable first and follow when we constructed that uh recursively sent partisan 4101 we did not uh compute uh those sets of uh of terminals that was because that though that grammar was so simple that that could just figure it out by staring at the code but if the grammar is more complex then you really want to compute this information otherwise you're not guaranteed that you're that already catch all the cases that the person needs to recognize so nullable means a non-terminal is nullable if that non-terminal can derive the empty string so obviously if you have an empty right hand side for for the non-terminal x then it's nullable but it's not just if the right hand side is empty but if maybe indirectly the empty string could be derived from that non-terminal the first set is the set of all terminals with which any string that can be derived from from x can start so um basically the language the sets of strings that can be derived from x let's look at all of those strings what are the the first terminals of those strings those would go into the first set what uh some authors do is they combine novel and first into uh into but they merge the nullable information into the first set so if a uh if a non-terminal is nullable then they simply include epsilon within the first set that's basically the same information just a slightly different presentation so if you look up parts table construction in youtube videos elsewhere or in in other textbooks very likely you might have uh first sets that contain epsilon but that would be uh whether the first set has epsilon or not that's exactly the same information as to whether the non-terminal is nullable or not now the follow set uh that's the set of terminals that can follow x in any derivation so if you have a derivation where in the middle of the derivation we still have that non-terminal x then whatever comes after that x what are the terminals that can come after x those are the terminals that need to go into the follow set and in some cases if if the non-terminal is is followed by in the file you might also need to put in the file explicitly into the follow set it doesn't hurt to always put it in there it becomes an issue if a non-terminal is nullable so that that you might have a parse decision or a a table look up a decision for which branch to go in the parser on end of file not all grammars have this the next example we're going to look at does not have that but it does uh it does happen now after we uh constructed nullable first and follow we need to generalize that information to an entire right-hand side uh i'm not explaining this explicitly that's um that's pretty obvious how this would work once you see it in the example now with this information we then produce what's called a predictive parse a table driven ll1 parser where we have uh two dimensions in the table uh we have a row for each non-terminal and the column for each terminal symbol and the entries in the parse table are grammar rules and so um if we have a grammar rule x arrow gamma and that's copied out of the book for some reason they uh they used the greek character gamma for that so if you have a grammar rule x arrow gamma then um for then in row x for each terminal t that's in the first set of gamma that's where we enter that grammar rule x arrow gamma if gamma is nullable then we enter the grammar rule in each column that's in the follow set of gamma and um and similarly uh if the if the follow set contains end of file you might need the end uh enter a grammar rule into the column for end of file as well and if after all this the table contains multiple uh any tables locked in the table contains multiple grammar rules then that means that this parser was not lr1 just like if you have multiple parse actions and then uh in an ll one or l zero uh uh parse table then uh you know that this was not uh you have a parse conflict you know that the grammar is not the llc lr0 or lr1 now uh let's see how this works in an example so we we have the simple grammar is z is a little d or x by y by z and z is the start symbol of the grammar y is either empty or a little c and x is a capital y or a little a so first we compute a level first and follow for nullable y obviously is nullable because that um that right-hand side is empty x is not level 2 because the first grammar rule for x only has y on the right hand side and y is nullable so therefore x is nullable as well z is not nullable because the first gram rule for z has has a token there and the second grammar rule for z has a recursive use of z and the base case of the recursion means that eventually we would need to have a little b there as well now for the first set uh wherever you have tokens that's obvious so let me get a laser pointer here so because of uh that grammar rule that says x is a little a we know that a must be in the first set of x because of the gram rule that says y is a little z a little c we know that c must must be in the first set of y in case of y there isn't really anything else so that's the entire first set of y but now with the grammar rule that says x can be a y that y can be notable that's because uh we said that x is now l2 but that y could also be a c and so therefore we also have c in the first set of x now how about uh capital z so here from the first grammar rule we know that little d must be in the first set of z from the second rule whatever is in the first set of x must also be in the first set of z and since x is nullable whatever is in the first set of y must also be in the first set of z and since y is nullable as well whatever comes after the y which happens to be z again must also be in the first set of of z so all three tokens a c and b are in the first set of z now for the follow set the um for for z as the uh as a start symbol it's easy uh z would only be follows by end of file right here so therefore we have a dollar in the follow set if you see this example in the book it does not uh talk about in the file explicitly because for this grammar we're not going to need a parse decision on the end of file if um now for x and y so for x we need to look at all the occurrences of x on anywhere in the grammar on the right hand side so this is the occurrence of x on the right hand side what what follows x right here what follows x is y so whatever is in the first set of y must be in the follow set of x and since y is nullable whatever is in the first set of z must also be in the follow set of x so so therefore the follow set of x would have all three tokens a c and d now for y we need to look at both occurrences of y in the grammar for the first occurrence y is followed by z so whatever is in the first set of z must be in the follow set of x so we add x a c and d to the follow set of uh uh of uh to default set of y we add ac b to the false set of y now we need to look at the second occurrence of y so here y occurs in the right most position in the grammar rule so we must have computed the follow set of x first and whatever was in the follow set of x must now also be in the fall set of y which in this example is again the same tokens a c and d so uh nothing really uh changes here um when i ask a questions like this on homeworks or or exams where people make the most errors isn't constructing the follow set so what i recommend is maybe maybe re-watch the last the last few minutes of this video and uh and work out work out the example yourself as you listen to the explanation maybe with stops in between watching it now once we have we have computed the first and follow set then we can produce the parse table so uh as i said previously we had each row in the parse table is for non-terminal each column corresponds to a terminal and um now let's just follow the algorithm so it says for each terminal in the first set of that right hand side so for this we need to generalize the notion of first and follow to an m to an entire right-hand side instead of just a non-terminal but um this is fairly obvious so let's look at the first grammar rule z arrow little d so the first set of that right hand side is just a little b obviously so we add this grammar rule into column d right here now uh the second grammar rule z is x y z what's the first set of that right hand side we it's the first set of x and since x is notable we need we also need to include the first set of y and since y is also nullable we also need to include the first set of z and so again we have all three tokens a c and b and so we add the grammar rule z arrow x y z into all three uh tail slots into the columns a c and d now for uh for y for the empty part if the right hand side is is nullable then we we add the grammar rule uh y arrow empty by error this right inside into all the table slots in the follow set of y the follow set of y is contains a c and d so therefore we add y error empty into all three columns here now y arrow c the first set of c is obviously just a c so we add y arrow c and column c for x that right hand side for x arrow y is again nullable because y is nullable so we need to add this grammar rule into each into each terminal that's in the follow set of x the following set of x is acb so we add x arrow y into each column for ac and p and finally x arrow a the first set of this is just a so we added in column a we didn't need to add to add anything in in the dollar column for in the file and that's because because of this in the book that's not included but uh i included it here because for some grammars uh you might have parse decisions on in the file especially if the start symbol might have a grammar rule for example that's nullable then [Music] then you might end up having to write an if condition that says if we have end of file then parse the uh like this else if we have some non in the file look at token uh parsed like that and so on now uh is this grammar ll1 no because we have three table slots with uh more than one grammar rule so this is not l1 by the way this is also unusual that this table is nearly full for uh most interesting grammars obviously the the uh it would not be that full this is just a simplistic example um yeah this is it for um for ll parsing uh the next uh slides are uh just was for summary for this module



Summary (11 min): https://youtu.be/qdWKPIr0SY4
when talking about parsing this one more uh brief topic that we should mention namely how is the parser included within the uh the software architecture of the compiler and in particular whether we have a single pass or a multi-pass compiler what we are building is a multi-pass compiler where name calls the parser the parser calls the lexical analyzer as needed to get one token after another and then eventually the parser returns the parse tree back to main and then main calls any other parse tree traversals semantic analysis uh intermediate code generation optimizers etc a single in a single pass compiler main simply calls the parser and the parser does everything else so um suppose we have this we have a single parse compiler a single pass compiler we might have a parser code like this where this is a c style function declaration we have a function declaration statement uh the semicolon is missing here we have uh i'll add this to the slide we have the function declaration non-terminal which is the return type of the function the function identifier a parameter list as a non-terminal and then after we've seen the parameter list at this point we might immediately add both the identifier for the function and the parameters into the symbol table so in the middle of that grammar rule we might have a scenaric action that says at the identifying the parameters into the symbol table and then we continue parsing we see the the left curly brace for the beginning of the function body a statement list for the function body the right curly brace and then after we we're done with the function body we there might be any remaining type checking a lot of the type checking would have happened while we've been parsing the statement list already but maybe any more type checking for the function body for example whether the return type of the function body is the same as the declared return type and then we immediately generate code and then we move on to the next declaration when i was working on my uh on my dissertation i was modifying uh g plus plus and so for figuring out how the cheapest plus compiler worked i was running the compiler front end uh in um or actually the the proper the actual compiler inside the debugger so the executable that you're calling is g plus plus but the actual compiler is an executable called cc one plus that's uh hidden somewhere in in an installation the directory uh and cc oneplus takes a c code on or c plus plus code on standard input and prints uh assembly a code on standard output and so i would be running cc one plus inside the debugger and so in the debugger i would type in a function declaration and as soon as i'm done with the function declaration and go on to the next open then the the compiler would immediately spit out the assembly code for that function and then i would type in the next function declaration so it's cheapest plus is a is truly a single pass compiler at least with the the parts that it had at the time with now with the the recursive descent parser that achieve with bus has now it may not be a single pass compiler anymore what are the pros and cons a single pass compiler requires less storage for data structures because you don't need to build an entire parse tree and but in exchange it's uh more time consuming because you need to build a tree it's reversed the same information multiple times the single pass the the multi-pass compiler also has a much cleaner architecture and it's uh much easier to work with a single pass compiler code is very difficult to read especially reading that the parser code in the on the [Music] reference on the resources page on moodle i have uh the c plus plus front end of or an old version of um of cheapest plus uh when i worked on this i think it was it's version 283 and so have a look at the the parser file in the file with extension i think it's parse.y extension y means it's the parser source file it is really hard to read reading uh reading uh cheapest plus source code is right up there with vulvan poetry for those of you who know hi hi this guide to the galaxy now to summarize the parser recognizes the context three syntactic structures in the token stream any language constructs that are not context-free need to be kept for scientific analysis that means if a cement if there are context sensitive properties that need to be checked the parser needs to be uh more liberal in what input it accepts for example it needs to accept input like x plus y and z and we describe the grammar use the grammar of the language using a context free grammar such as a bnf grammar and once we have the the grammar specification of the language then we could write either a bottom-up or top-down parser for a bottom-up parser nobody in their right mind really writes a bottom up parser by hand i i have one textbook where they explain how a bottom-up parser works by by actually going through the motions of uh of developing the source code by hand for a bottom-up parser but uh you normally don't do this you use the parser generator so if you're doing bottom-up parsing you would use a parser generator that translates that grammar into a tail driven parser and the parse engine is a push down automaton which essentially consists of a dfa and the stack plus a generic mechanism for interpreting the parse table and some of the parses that are out there that generate these bottom up parses would be yak bison yak by the way stands for yet another compiler compiler yak bison java cup or emiliac and the parser classes the grammar classes and correspondingly parser classes that are of interest are ls0 slr 1l1 and then lrk for larger k but for practical purposes we normally don't build llk parses because those parts tables will become way too big now for top-down parsing you could write a indirect and recursive descent parser and that requires that um that you first bring the grammar into the format with either bottom-up parsing the top-down parts and you need to bring the grammar into format so it can be recognized with that parsing technology in case of the top-down parser it means getting rid of electric version common left factors and then it could either have a table present predictive parser or or a parser code generated by an llk parser for common l uh llk parser generators the most widely used one is antler the parser technology for intel was developed while i was in grad school i was in in the computer science department at purdue and and i can't think of his name right now um a student in the electrical engineering department developed this um this antler parts technology and uh terence park and he's still working on this he is now faculty uh somewhere in florida uc davis maybe and he keeps maintaining inputs it's the best llk for the generator out there it works with the target for a variety of different languages and in other possibilities java cc which was the first part of the generator that was written for java and the grant interesting grammar classes ll zero if you want to have a parser that works interactively or ll1 with one token lookahead and for an llk parser you really don't want to write that code by hand anymore that gets too complicated you you want to use a parser generator by the way what uh those llk parser generators uh spit out is literally code that look by the structure of that code looks like a handwritten recursive descent parser except that there's a lot of additional bookkeeping in between for maintaining the potentially arbitrary look ahead this concludes uh the material for um for the parser module the next module is going to be a significant analysis

